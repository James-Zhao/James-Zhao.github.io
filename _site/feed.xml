<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>James Zhao</title>
    <description>The people who are crazy enough to think they can change the world are the ones who do.</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 18 Jun 2017 23:25:17 +0800</pubDate>
    <lastBuildDate>Sun, 18 Jun 2017 23:25:17 +0800</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title>Java垃圾回收</title>
        <description>&lt;p&gt;这篇博客是对Java垃圾回收的总结，主要是对&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;以及后续的三篇博客的翻译。我把这四篇博客翻译到这一篇博客里，把参考的其他博客的链接附在文章末尾。&lt;/p&gt;

&lt;h2 id=&quot;java-garbage-collection-introduction&quot;&gt;Java Garbage Collection Introduction&lt;/h2&gt;
&lt;p&gt;在Java中，分配和回收对象的内存空间是在JVM中、由垃圾回收机制自动完成的。不像C语言，使用Java的开发人员不需要写代码来进行垃圾回收。Java之所以如此流行以及能够帮助程序员更好的开发，这是其中一个原因。&lt;/p&gt;

&lt;p&gt;这个系列一共有四篇博客来介绍Java的垃圾回收，分别是：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://javapapers.com/java/how-java-garbage-collection-works/&quot;&gt;How Java Garbage Collection Works?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://javapapers.com/java/types-of-java-garbage-collectors/&quot;&gt;Types of Java Garbage Collectors&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-monitoring-and-analysis/&quot;&gt;Java Garbage Collection Monitoring and Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这一节是这篇博客的第一部分。主要内容是解释一些基础的术语，比如JDK, JVM, JRE, HotSpot VM，然后理解JVM的整体架构和Java堆内存结构。理解这些概念对学习Java垃圾回收很有帮助。&lt;/p&gt;

&lt;h3 id=&quot;key-java-terminologies&quot;&gt;Key Java Terminologies&lt;/h3&gt;
&lt;p&gt;Java API：Java API是帮助程序员进行Java开发的一系列封装库的集合。&lt;/p&gt;

&lt;p&gt;Java Development Kit (JDK)：JDK是一套帮助程序员进行Java开发的工具，这些工具主要是进行编译、运行、打包、分配和监视Java程序。&lt;/p&gt;

&lt;p&gt;Java Virtual Machine (JVM)：JVM是一个抽象的计算机。Java程序是针对JVM规范来写的。对不同的操作系统来说，JVM有所不同，它主要作用是把Java指令翻译为操作系统的指令，并且执行它们。JVM能够保证Java代码独立于平台。&lt;/p&gt;

&lt;p&gt;Java Runtime Environment (JRE)：JRE包括JVM实现和Java API。&lt;/p&gt;

&lt;h3 id=&quot;java-hotspot-virtual-machine&quot;&gt;Java HotSpot Virtual Machine&lt;/h3&gt;
&lt;p&gt;不同的JVM在垃圾回收的实现上可能有点不一样。 在SUN被收购之前，Oracle有JRockit JVM，并且在SUN收购之后，Oracle获得了HotSpot JVM。目前Oracle维护了两个JVM实现，并且已经声明，在一段时间内这两个JVM实现将被合并到一个。&lt;/p&gt;

&lt;p&gt;HotSpot JVM是标准Oracle SE平台的核心组件。在这个垃圾回收教程中，我们将看到基于HotSpot虚拟机的垃圾回收原理。&lt;/p&gt;

&lt;h3 id=&quot;jvm-architecture&quot;&gt;JVM Architecture&lt;/h3&gt;
&lt;p&gt;下图总结了JVM的关键组件。在JVM架构中，两个主要涉及到垃圾回收的组件是堆内存(heap memory)和垃圾回收器(garbage collector)。堆内存是运行时数据所在的区域，这个区域存着对象实例，垃圾回收器也在这个区域内操作。我们现在看看这些东西如何适应到更大的方案。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201706/798143-20170603114328118-672196757.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;runtime-data-area&quot;&gt;Runtime Data Area&lt;/h3&gt;
&lt;p&gt;正如虚拟机规范所说的那样，JVM中的内存分为5个虚拟的区域:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Method Area：也被称为非堆区域(在HotSpot JVM的实现当中)。它被分为两个主要的子区域：持久代和代码缓存。前者会存储包括类定义，结构，字段，方法(数据及代码)以及常量在内的类相关数据。后者是用来存储编译后的代码。编译后的代码就是本地代码，它是由JIT编译器生成的，这个编译器是Oracle HotSpot JVM所特有的。&lt;/li&gt;
  &lt;li&gt;Heap Memory：后续会详细介绍。&lt;/li&gt;
  &lt;li&gt;Java Stacks：和Java类中的方法密切相关，它会存储局部变量以及方法调用的中间结果及返回值，Java中的每个线程都有自己专属的栈，这个栈是别的线程无法访问的。&lt;/li&gt;
  &lt;li&gt;PC Registers：特定线程的程序计数器，包含JVM正在执行的指令的地址(如果是本地方法的话它的值则未定义)&lt;/li&gt;
  &lt;li&gt;Native Method Stack：用于本地方法(非Java代码)，按线程分配&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;java-heap-memory&quot;&gt;Java Heap Memory&lt;/h3&gt;
&lt;p&gt;理解Java Heap Memory在JVM模型中的角色很重要。在运行期间，Java实例保存在heap memory区域。当一个对象没有引用指向它时，它就需要从内存里被清除。在垃圾回收的过程中，这些对象在heap memory里被清除，然后这些空间能够再被利用。heap memory主要有三个区域：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;年轻代。年轻代分三个区：Eden区（任何进去内存区域的对象实例都要经过eden区）、Survivor0区（从eden区过来S0区的、稍微老一点的对象）、Survivor1区（从S0区过来、稍微老一点的对象）&lt;/li&gt;
  &lt;li&gt;年老代。这个区主要是从S1过来的对象。&lt;/li&gt;
  &lt;li&gt;持久代。这个区包含一些元数据信息比如，类、方法等。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;更新：持久代区域已经从Java SE 8移除。取代它的是另一个内存区域也被称为元空间(本地堆内存中的一部分)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201706/798143-20170603115520993-468351782.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-java-garbage-collection-works&quot;&gt;How Java Garbage Collection Works?&lt;/h2&gt;
&lt;p&gt;这个系列博客主要是帮助理解Java回收的基本概念以及其如何工作。这部分是这个系列的第二部分，希望你有阅读过第一部分&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Java垃圾回收是一个自动处理的过程，目的是管理程序运行时的内存问题。通过这种方式，JVM能够自动减轻程序员在分配和释放内存资源的问题。&lt;/p&gt;

&lt;h3 id=&quot;java-garbage-collection-gc-initiation&quot;&gt;Java Garbage Collection GC Initiation&lt;/h3&gt;
&lt;p&gt;垃圾回收作为一个自动的过程，程序员不用在代码里显式地启动垃圾回收机制。System.gc()和Runtime.gc()能够显式地让JVM执行垃圾回收过程。&lt;/p&gt;

&lt;p&gt;虽然这个机制能够让程序员启动垃圾回收，但是这个责任还是在JVM上。JVM能够选择拒绝你的请求，所以我们没法保证这两个命令肯定会执行垃圾回收。JVM会通过eden区的可用内存来判断是否执行垃圾回收。JVM规范把这个选择留给了JVM的具体实现，所以这些实现的细节对不同的JVM来说也不一样。&lt;/p&gt;

&lt;p&gt;毫无疑问的是，垃圾回收机制不能强制执行。我刚刚发现了一个调用System.gc()的应用场景，看这篇文章就知道什么时候&lt;a href=&quot;http://javapapers.com/core-java/system-gc-invocation-a-suitable-scenario/&quot;&gt;调用System.gc()是合适的&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;java-garbage-collection-process&quot;&gt;Java Garbage Collection Process&lt;/h3&gt;
&lt;p&gt;垃圾回收是这样一个过程：回收不再使用的内存空间，让这些空间能够被将来的实例对象所用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201706/798143-20170603210309758-1962301044.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;eden-space&quot;&gt;Eden Space&lt;/h4&gt;
&lt;p&gt;当一个实例被创建的时候，它首先存在堆内存区域的年轻代的eden区。&lt;/p&gt;

&lt;p&gt;注意：假如你不懂这些术语，建议你去读&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;，这篇博客把内存模型、JVM架构和一些专业术语解释的很详细。&lt;/p&gt;

&lt;h4 id=&quot;survivor-space-s0-and-s1&quot;&gt;Survivor Space (S0 and S1)&lt;/h4&gt;
&lt;p&gt;作为次要垃圾回收(minor GC)周期的一部分，那些仍然被引用的对象会从eden区被搬到survivor的S0区。同样的过程，垃圾回收器会扫描S0区，然后把实例搬到S1区。&lt;/p&gt;

&lt;p&gt;那些没有被引用的对象会被标记。通过不同的垃圾回收器(一共有四类垃圾回收器，后续的教程会讲这个)来决定这些被标记的对象在内存中被移除还是在单独的进程中来完成。&lt;/p&gt;

&lt;h4 id=&quot;old-generation&quot;&gt;Old Generation&lt;/h4&gt;
&lt;p&gt;年老代或者持久代是堆内存中的第二个逻辑部分。当垃圾回收器执行次要垃圾回收周期的时候，那些仍在S1区存活的实例将被移到年老代，S1区域中没有被引用的实例将会被标记以便清除。&lt;/p&gt;

&lt;h4 id=&quot;major-gc&quot;&gt;Major GC&lt;/h4&gt;
&lt;p&gt;在垃圾回收的过程中，年老代是实例对象生命周期的最后一个环节。Major GC是扫描年老代的堆内存的垃圾回收过程。如果有对象没有被引用，那它们将被标记以便清除，如果有被引用，那它们将继续留在年老代。&lt;/p&gt;

&lt;h4 id=&quot;memory-fragmentation&quot;&gt;Memory Fragmentation&lt;/h4&gt;
&lt;p&gt;一旦实例对象在堆内存中被删除，那它们所占的区域就又可以被将来的实例对象使用。这些空的区域在内存中将会形成很多碎片。为了更快的为实例分配内存，我们应该解决这个碎片问题。根据不同垃圾回收器的选择，被回收的内存将在回收的过程同时或者在GC另外独立的过程中压缩整合。&lt;/p&gt;

&lt;h3 id=&quot;finalization-of-instances-in-garbage-collection&quot;&gt;Finalization of Instances in Garbage Collection&lt;/h3&gt;
&lt;p&gt;就在清除一个对象并回收它的内存空间之前，垃圾回收器将会调用各个实例的finalize()方法，这样实例对象就有机会可以释放掉它占用的资源。尽管finalize()方法是保证在回收内存空间之前执行的，但是对具体的执行时间和执行顺序是没有任何保证的。多个实例之间的finalize()执行顺序是不能提前预知的，甚至有可能它们是并行执行的。程序不应该预先假设实例执行finalize()的顺序，也不应该使用finalize()方法来回收资源。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在finalize过程中抛出的任何异常都默认被忽略掉了，同时该对象的销毁过程被取消&lt;/li&gt;
  &lt;li&gt;JVM规范并没有讨论关于弱引用的垃圾回收，并且是明确声明的。具体的细节留给实现者决定。&lt;/li&gt;
  &lt;li&gt;垃圾回收是由守护进程执行的&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;when-an-object-becomes-eligible-for-garbage-collection&quot;&gt;When an object becomes eligible for garbage collection?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;任何不能被活着的线程用到的实例&lt;/li&gt;
  &lt;li&gt;不能被其他对象用到的循环引用对象&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Java中有&lt;a href=&quot;http://javapapers.com/core-java/java-weak-reference/&quot;&gt;多种不同的引用类型&lt;/a&gt;。实例的可回收性取决于它的引用类型。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;引用&lt;/th&gt;
      &lt;th&gt;垃圾回收&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;强引用&lt;/td&gt;
      &lt;td&gt;不适合被垃圾回收&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;软引用&lt;/td&gt;
      &lt;td&gt;可能会被回收，但是是在最后被回收&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;弱引用&lt;/td&gt;
      &lt;td&gt;适合被回收&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;虚引用&lt;/td&gt;
      &lt;td&gt;适合被回收&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在编译期间，Java编译器有优化技术来选择把null值赋给一个实例，从而把这个实例标记为可回收。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Animal {
    public static void main(String[] args) {
        Animal lion = new Animal();
        System.out.println(&quot;Main is completed.&quot;);
    }

    protected void finalize() {
        System.out.println(&quot;Rest in Peace!&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在上面的类中，lion实例在初始化后就没被用过了，所以Java编译器在lion实例化后把lion赋值为null而作为一个优化的手段。这样finlizer方法可能会在Main方法之前打印结果。我们不能确定地保证结果，因为这和JVM的具体实现以及运行时的内存有关。但是我们能知道的一点是：编译器发现一个实例在之后的程序中不再被引用时可以选择提前释放实例的内存。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里有个实例何时变成可回收更好的例子。实例所有的属性可以被存储在寄存器中，之后可以从寄存器中读取这些属性值。在任何情况下，这些值将来都不会被写回到实例对象中。尽管这些值在未来还是被使用到了，但是实例对象依然可以被标记为可回收的。（注：这个例子我没看懂）&lt;/li&gt;
  &lt;li&gt;我们可以简单地认为，当一个实例被赋值为null时就可以被回收。我们也可以像上面提到的、复杂地考虑这个问题。这些都是由JVM的具体实现决定的。其目标都是希望留下最少的痕迹、提高响应性能和增大吞吐量。为了能够达到这些目的，JVM实现者可以在垃圾回收中选择更好的模式或算法来回收内存。&lt;/li&gt;
  &lt;li&gt;当finalize()被调用的时候，JVM释放掉当前线程的所有同步块。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;example-program-for-gc-scope&quot;&gt;Example Program for GC Scope&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Class GCScope {
	GCScope t;
	static int i = 1;

	public static void main(String args[]) {
		GCScope t1 = new GCScope();
		GCScope t2 = new GCScope();
		GCScope t3 = new GCScope();

		// No Object Is Eligible for GC

		t1.t = t2; // No Object Is Eligible for GC
		t2.t = t3; // No Object Is Eligible for GC
		t3.t = t1; // No Object Is Eligible for GC

		t1 = null;
		// No Object Is Eligible for GC (t3.t still has a reference to t1)

		t2 = null;
		// No Object Is Eligible for GC (t3.t.t still has a reference to t2)

		t3 = null;
		// All the 3 Object Is Eligible for GC (None of them have a reference.
		// only the variable t of the objects are referring each other in a
		// rounded fashion forming the Island of objects with out any external
		// reference)
	}

	protected void finalize() {
		System.out.println(&quot;Garbage collected from object&quot; + i);
		i++;
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;example-program-for-gc-outofmemoryerror&quot;&gt;Example Program for GC OutOfMemoryError&lt;/h3&gt;
&lt;p&gt;垃圾回收不保证内存的安全问题，粗心的代码会造成内存溢出。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import java.util.LinkedList;
import java.util.List;

public class GC {
	public static void main(String[] main) {
		List l = new LinkedList();
		// Enter infinite loop which will add a String to the list: l on each
		// iteration.
		do {
			l.add(new String(&quot;Hello, World&quot;));
		} while (true);
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;输出为&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space
	at java.util.LinkedList.linkLast(LinkedList.java:142)
	at java.util.LinkedList.add(LinkedList.java:338)
	at com.javapapers.java.GCScope.main(GCScope.java:12)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;types-of-java-garbage-collectors&quot;&gt;Types of Java Garbage Collectors&lt;/h2&gt;
&lt;p&gt;在这个教程中，我们将学习各种各样的垃圾回收器。Java垃圾回收是一个自动处理的过程，目的是管理程序运行时的内存问题。这是这个系列教程的第三部分，在前面两个部分，我们学习了&lt;a href=&quot;http://javapapers.com/java/how-java-garbage-collection-works/&quot;&gt;How Java Garbage Collection Works&lt;/a&gt;，这部分很有意思，我建议你完整读一遍。而第一部分&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;中，我们学习了JVM的架构、堆内存模型和Java相关的术语。&lt;/p&gt;

&lt;p&gt;Java有四种垃圾回收器：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Serial Garbage Collector&lt;/li&gt;
  &lt;li&gt;Parallel Garbage Collector&lt;/li&gt;
  &lt;li&gt;CMS Garbage Collector&lt;/li&gt;
  &lt;li&gt;G1 Garbage Collector&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201706/798143-20170604100113180-430238765.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这四种垃圾回收器各有自己的利弊。更重要的是，我们程序员可以选择JVM用哪种垃圾回收器。我们一般是通过设置JVM参数来选择垃圾回收器。各个垃圾回收器在不同应用场景下的效率会有很大的差异。因此了解各种不同类型的垃圾回收器以及它们的应用场景是非常重要的。&lt;/p&gt;

&lt;h3 id=&quot;serial-garbage-collector&quot;&gt;Serial Garbage Collector&lt;/h3&gt;
&lt;p&gt;串行垃圾回收器控制所有的应用线程。它是为单线程环境设计的，只使用一个线程来执行垃圾回收工作。它的工作方式是暂停所有应用线程来执行垃圾回收工作，这种方式不适用于服务器的应用环境。它最适用的是简单的命令行程序。&lt;/p&gt;

&lt;p&gt;使用-XX:+UseSerialGC JVM参数来开启使用串行垃圾回收器。&lt;/p&gt;

&lt;h3 id=&quot;parallel-garbage-collector&quot;&gt;Parallel Garbage Collector&lt;/h3&gt;
&lt;p&gt;并行垃圾回收器也称作基于吞吐量的回收器。它是JVM的默认垃圾回收器。与Serial垃圾回收器不同的是，它使用多个线程来执行垃圾回收工作。和Serial回收器一样，它在执行垃圾回收工作是也需要暂停所有应用线程。&lt;/p&gt;

&lt;h3 id=&quot;cms-garbage-collector&quot;&gt;CMS Garbage Collector&lt;/h3&gt;
&lt;p&gt;并发标记清除(Concurrent Mark Sweep,CMS)垃圾回收器，使用多个线程来扫描堆内存并标记可被清除的对象，然后清除标记的对象。CMS垃圾回收器只在下面这两种情形下暂停工作线程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在老年代中标记引用对象的时候&lt;/li&gt;
  &lt;li&gt;在并行垃圾回收的过程中堆内存中有变化发生&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对比与并行垃圾回收器，CMS回收器使用更多的CPU来保证更高的吞吐量。如果我们可以有更多的CPU用来提升性能，那么CMS垃圾回收器是比并行回收器更好的选择。&lt;/p&gt;

&lt;p&gt;使用-XX:+UseParNewGC JVM参数来开启使用CMS垃圾回收器。&lt;/p&gt;

&lt;h3 id=&quot;g1-garbage-collector&quot;&gt;G1 Garbage Collector&lt;/h3&gt;
&lt;p&gt;G1垃圾回收器应用于大的堆内存空间。它将堆内存空间划分为不同的区域，对各个区域并行地做回收工作。G1在回收内存空间后还立即堆空闲空间做整合工作以减少碎片。CMS却是在全部停止(stop the world,STW)时执行内存整合工作。G1会根据各个区域的垃圾数量来对区域评判优先级。&lt;/p&gt;

&lt;p&gt;使用-XX:UseG1GC JVM参数来开启使用G1垃圾回收器。&lt;/p&gt;

&lt;h4 id=&quot;java-8-improvement&quot;&gt;Java 8 Improvement&lt;/h4&gt;
&lt;p&gt;在使用G1垃圾回收器时，开启使用-XX:+UseStringDeduplacaton JVM参数。它会通过把重复的String值移动到同一个char[]数组来优化堆内存占用。这是Java 8 u 20引入的选项。&lt;/p&gt;

&lt;p&gt;以上给出的四个Java垃圾回收器，在什么时候使用哪一个取决于应用场景，硬件配置和吞吐量要求。&lt;/p&gt;

&lt;h3 id=&quot;garbage-collection-jvm-options&quot;&gt;Garbage Collection JVM Options&lt;/h3&gt;
&lt;p&gt;下面是与Java垃圾回收相关、比较重要的JVM选项。&lt;/p&gt;

&lt;h4 id=&quot;type-of-garbage-collector-to-run&quot;&gt;Type of Garbage Collector to run&lt;/h4&gt;
&lt;p&gt;选项 | 描述
—-|——
-XX:+UseSerialGC|Serial Garbage Collector
-XX:+UseParallelGC|Parallel Garbage Collector
-XX:+UseConcMarkSweepGC|CMS Garbage Collector
-XX:ParallelCMSThreads=|CMS Collector – number of threads to use
-XX:+UseG1GC|G1 Gargbage Collector&lt;/p&gt;

&lt;h4 id=&quot;gc-optimization-options&quot;&gt;GC Optimization Options&lt;/h4&gt;
&lt;p&gt;选项 | 描述
—-|——
-Xms|Initial heap memory size
-Xmx|Maximum heap memory size
-Xmn|Size of Young Generation
-XX:PermSize|Initial Permanent Generation size
-XX:MaxPermSize|Maximum Permanent Generation size&lt;/p&gt;

&lt;h4 id=&quot;example-usage-of-jvm-gc-options&quot;&gt;Example Usage of JVM GC Options&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java -Xmx12m -Xms3m -Xmn1m -XX:PermSize=20m -XX:MaxPermSize=20m -XX:+UseSerialGC -jar java-application.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;java-garbage-collection-monitoring-and-analysis&quot;&gt;Java Garbage Collection Monitoring and Analysis&lt;/h2&gt;
&lt;p&gt;这篇教程主要介绍垃圾回收机制的监控和分析，我们会用到一个工具来监控一个Java应用的垃圾回收过程。如果你是一个新手，你最好把这系列博客的前几篇都看一下，你可以从&lt;a href=&quot;http://javapapers.com/java/java-garbage-collection-introduction/&quot;&gt;Java Garbage Collection Introduction&lt;/a&gt;开始。&lt;/p&gt;

&lt;h3 id=&quot;java-garbage-collection-monitoring-and-analysis-tools&quot;&gt;Java Garbage collection monitoring and analysis tools&lt;/h3&gt;
&lt;p&gt;下面说的工具各自都有利弊，我们可以通过选择合适的工具来进行分析以此提高Java应用的性能。我们这篇教程主要用Java VisualVM。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Java VisualVM&lt;/li&gt;
  &lt;li&gt;Naarad&lt;/li&gt;
  &lt;li&gt;GCViewer&lt;/li&gt;
  &lt;li&gt;IBM Pattern Modeling and Analysis Tool for Java Garbage Collector&lt;/li&gt;
  &lt;li&gt;HPjmeter&lt;/li&gt;
  &lt;li&gt;IBM Monitoring and Diagnostic Tools for Java – Garbage Collection and Memory&lt;/li&gt;
  &lt;li&gt;Visualizer&lt;/li&gt;
  &lt;li&gt;Verbose GC Analyzer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;java-visualvm&quot;&gt;Java VisualVM&lt;/h3&gt;
&lt;p&gt;Java VisualVM可以通过Java SE SDK来免费安装使用。看看你的Java JDK安装路径，就在\Java\jdk1.8.0\bin路径下。当然还有很多其他的工具，jvisualvm只是其中之一。&lt;/p&gt;

&lt;p&gt;Java VisualVM提供了一个包含Java程序信息的可视化接口。它包含很多其他的工具且集成到这一个。现在像JConsole, jstat, jinfo, jstack, and jmap都被集成到Java VisualVM中了。&lt;/p&gt;

&lt;p&gt;Java VisualVM主要用来：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;generate and analyze heap memory dumps&lt;/li&gt;
  &lt;li&gt;view and operate on MBeans&lt;/li&gt;
  &lt;li&gt;monitor garbage collection&lt;/li&gt;
  &lt;li&gt;memory and CPU profiling&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;launch-visualvm&quot;&gt;Launch VisualVM&lt;/h4&gt;
&lt;p&gt;JDK目录下就有Java VisualVM。&lt;/p&gt;

&lt;h4 id=&quot;install-visual-gc-plugin&quot;&gt;Install Visual GC Plugin&lt;/h4&gt;
&lt;p&gt;我们需要安装一个可视化插件，以便观察Java GC过程。&lt;/p&gt;

&lt;h4 id=&quot;monitor-gc&quot;&gt;Monitor GC&lt;/h4&gt;
&lt;p&gt;启动你的Java应用，Java VisualVM会自动开始监控，并把结果展示在Java VisualVM可视化接口中。你可以选择不同的组件来观察你感兴趣的指标。&lt;/p&gt;

&lt;h2 id=&quot;参考的其他博客&quot;&gt;参考的其他博客&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/yechaodechuntian/article/details/40341975&quot;&gt;java8的JVM持久代——何去何从？&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Jun 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//Java%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6</link>
        <guid isPermaLink="true">http://localhost:4000//Java%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6</guid>
        
        
      </item>
    
      <item>
        <title>PostgreSQL新手入门</title>
        <description>&lt;p&gt;这篇博客主要对PostgreSQL进行总结，内容偏基础。&lt;/p&gt;

&lt;p&gt;这里先附上一个PostgreSQL的中文资源：&lt;a href=&quot;http://www.kuqin.com/postgreSQL8.1_doc/&quot;&gt;PostgreSQL 8.1 中文文档&lt;/a&gt;。英文不好的同学可以看看这个。&lt;/p&gt;

&lt;h2 id=&quot;安装postgresql&quot;&gt;安装PostgreSQL&lt;/h2&gt;
&lt;h5 id=&quot;安装postgresql客户端&quot;&gt;安装PostgreSQL客户端&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install postgresql-client
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h5 id=&quot;安装postgresql服务器&quot;&gt;安装PostgreSQL服务器&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install postgresql
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;安装图形管理界面&quot;&gt;安装图形管理界面&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install pgadmin3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;postgresql添加新用户和新数据库&quot;&gt;PostgreSQL添加新用户和新数据库&lt;/h2&gt;
&lt;h3 id=&quot;方法1postgresql控制台&quot;&gt;方法1：PostgreSQL控制台&lt;/h3&gt;
&lt;h5 id=&quot;连接数据库&quot;&gt;连接数据库&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo su - postgres
psql -U user -d dbname
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;第一步是切换到postgres用户，第二步是登录到PostgreSQL控制台。当你直接敲psql登录时，实际上是以同名数据库用户的身份登录数据库。&lt;/p&gt;

&lt;p&gt;如果一切正常，系统提示符会变为”postgres=#”，表示这时已经进入了数据库控制台。以下的命令都在控制台内完成。&lt;/p&gt;

&lt;h5 id=&quot;设置系统用户密码&quot;&gt;设置系统用户密码&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\password postgres
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;为postgres用户设置密码，敲下这个命令后，控制台会提示你让你输入新的密码。注意，这里设置的是Linux系统用户postgresql的密码，并非数据库用户的密码。&lt;/p&gt;

&lt;h5 id=&quot;创建数据库用户&quot;&gt;创建数据库用户&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE USER dbuser WITH PASSWORD 'password';
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建数据库&quot;&gt;创建数据库&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE DATABASE exampledb OWNER dbuser;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;创建用户数据库，这里为exampledb，并指定所有者为dbuser。&lt;/p&gt;

&lt;h5 id=&quot;所有权赋予用户&quot;&gt;所有权赋予用户&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;退出控制台&quot;&gt;退出控制台&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\q
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;方法2使用shell命令行&quot;&gt;方法2：使用shell命令行&lt;/h3&gt;
&lt;p&gt;添加新用户和新数据库，除了在PostgreSQL控制台内，还可以在shell命令行下完成。这是因为PostgreSQL提供了命令行程序createuser和createdb。还是以新建用户dbuser和数据库exampledb为例。&lt;/p&gt;

&lt;h5 id=&quot;创建数据库用户dbuser并指定其为超级用户&quot;&gt;创建数据库用户dbuser，并指定其为超级用户。&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -u postgres createuser --superuser dbuser
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h5 id=&quot;登录数据库控制台设置dbuser用户的密码完成后退出控制台&quot;&gt;登录数据库控制台，设置dbuser用户的密码，完成后退出控制台。&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -u postgres psql
\password dbuser
\q
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h5 id=&quot;创建数据库exampledb并指定所有者为dbuser&quot;&gt;创建数据库exampledb，并指定所有者为dbuser&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo -u postgres createdb -O dbuser exampledb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;登录数据库&quot;&gt;登录数据库&lt;/h2&gt;
&lt;p&gt;添加新用户和新数据库以后，就要以新用户的名义登录数据库，这时使用的是psql命令。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;上面命令的参数含义如下：-U指定用户，-d指定数据库，-h指定服务器，-p指定端口。&lt;/p&gt;

&lt;p&gt;输入上面命令以后，系统会提示输入dbuser用户的密码。输入正确，就可以登录控制台了。&lt;/p&gt;

&lt;h2 id=&quot;控制台命令&quot;&gt;控制台命令&lt;/h2&gt;
&lt;p&gt;除了前面已经用到的\password命令（设置密码）和\q命令（退出）以外，控制台还提供一系列其他命令。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\h：查看SQL命令的解释，比如\h select。
\?：查看psql命令列表。
\l：列出所有数据库。
\c [database_name]：连接其他数据库。
\d：列出当前数据库的所有表格。
\d [table_name]：列出某一张表格的结构。
\du：列出所有用户。
\e：打开文本编辑器。
\conninfo：列出当前数据库和连接的信息。
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&quot;数据库操作&quot;&gt;数据库操作&lt;/h2&gt;
&lt;p&gt;基本的数据库操作，就是使用一般的SQL语言。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 创建新表
CREATE TABLE user_tbl(name VARCHAR(20), signup_date DATE);
# 插入数据
INSERT INTO user_tbl(name, signup_date) VALUES('张三', '2013-12-22');
# 选择记录
SELECT * FROM user_tbl;
# 更新数据
UPDATE user_tbl set name = '李四' WHERE name = '张三';
# 删除记录
DELETE FROM user_tbl WHERE name = '李四' ;
# 添加栏位
ALTER TABLE user_tbl ADD email VARCHAR(40);
# 更新结构
ALTER TABLE user_tbl ALTER COLUMN signup_date SET NOT NULL;
# 更名栏位
ALTER TABLE user_tbl RENAME COLUMN signup_date TO signup;
# 删除栏位
ALTER TABLE user_tbl DROP COLUMN email;
# 表格更名
ALTER TABLE user_tbl RENAME TO backup_tbl;
# 删除表格
DROP TABLE IF EXISTS backup_tbl;
# 给一个字段设置缺省值
ALTER TABLE [表名] ALTER COLUMN [字段名] SET DEFAULT [新的默认值];
# 去除缺省值
ALTER TABLE [表名] ALTER COLUMN [字段名] DROP DEFAULT;
# 更改字段类型
ALTER TABLE ALTER COLUMN TYPE;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;处理导出的数据中文显示乱码问题&quot;&gt;处理导出的数据中文显示乱码问题&lt;/h2&gt;
&lt;p&gt;PostgreSQL本身提供的copy命令可以将数据导成csv格式。但是在某些Linux环境下导出的csv文件，传输到windows环境下打开中文却显示乱码。&lt;/p&gt;

&lt;h3 id=&quot;解决方法1&quot;&gt;解决方法1&lt;/h3&gt;
&lt;p&gt;一般PostgreSQL建库都是用的UTF8字符集，在UTF8字符集情况下如果中文不能正常显示，可以设置客户端字符集，修改成”GBK” ，命令如下&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--修改客户端字符集
postgres=# show client_encoding;
 client_encoding
-----------------
 UTF8
(1 row)

postgres=# set client_encoding='GBK';
SET

--copy 导出数据到 GBK 编码类型的 csv 文件
skytf=# copy skytf.test_2 to '/home/postgres/script/tf/skytf.test_2.csv' with csv header;
COPY 1000000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;解决方法2&quot;&gt;解决方法2&lt;/h3&gt;
&lt;p&gt;iconv是linux命令，用来转换文件的编码的，手册解释如下”Convert encoding of given files from one encoding to another”，我们可以使用iconv命令转换文件的编码，如果 utf8编码的文件中文显示为乱码，可以使用iconv命令将UTF8格式文件转换成gb18030。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--导出数据到 utf8 编码类型文件。
skytf=# set client_encoding='UTF8';
SET

skytf=# show client_encoding;
 client_encoding
-----------------
 UTF8
(1 row)

skytf=# copy skytf.test_2 to '/home/postgres/script/tf/skytf.test_2.csv' with csv header;
COPY 1000000

--将文件编码由 utf8 转换成 gb18030
iconv -f utf-8 -t gb18030 skytf.test_2.csv -o skytf.test_2_gbk.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;文件导入导出&quot;&gt;文件导入导出&lt;/h2&gt;
&lt;h5 id=&quot;文件导出&quot;&gt;文件导出&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\copy table to data.csv  delimiter as ‘,’ csv quote as ‘”‘
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;文件导入&quot;&gt;文件导入&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\copy  table from data.csv delimiter as ‘,’ csv quote as ‘”‘
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;参考博客&quot;&gt;参考博客&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2013/12/getting_started_with_postgresql.html&quot;&gt;PostgreSQL新手入门&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://francs3.blog.163.com/blog/static/4057672720120694224195/&quot;&gt;PostgreSQL: 如何处理导出的数据中文显示乱码问题？  &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 24 May 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//PostgreSQL%E6%96%B0%E6%89%8B%E5%85%A5%E9%97%A8</link>
        <guid isPermaLink="true">http://localhost:4000//PostgreSQL%E6%96%B0%E6%89%8B%E5%85%A5%E9%97%A8</guid>
        
        
      </item>
    
      <item>
        <title>Java String</title>
        <description>&lt;p&gt;这篇博客是对String的总结，注意，不是系统地讲解String方方面面，想学String的全部知识，看看Java编程思想的字符串这一章就行，这篇博客只是对String一些细节问题的总结。整篇博客会比较零散，我尽量系统地总结好，而且会持续更新。具体引用别人博客的地方，我会附上链接。&lt;/p&gt;

&lt;h2 id=&quot;string-stringbuffer-stringbuilder&quot;&gt;String, StringBuffer, StringBuilder&lt;/h2&gt;
&lt;h3 id=&quot;string-stringbuffer-stringbuilder三者的区别&quot;&gt;String, StringBuffer, StringBuilder三者的区别&lt;/h3&gt;
&lt;p&gt;Java字符串处理离不开这三个类。这里总结下这三个类的区别，这里参考了&lt;a href=&quot;http://www.cnblogs.com/A_ming/archive/2010/04/13/1711395.html&quot;&gt;String、StringBuffer与StringBuilder之间区别&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;执行效率&quot;&gt;执行效率&lt;/h4&gt;
&lt;p&gt;在执行效率上StringBuilder &amp;gt; StringBuffer &amp;gt; String。但是也有例外，详情见下面的特殊的例子。&lt;/p&gt;

&lt;h4 id=&quot;可变vs不可变&quot;&gt;可变vs不可变&lt;/h4&gt;
&lt;p&gt;这点其实可以解释这三者执行效率的不同。主要原因是String是不可变类（有兴趣的同学可以看看String源码，其值是private final char value[]定义的），其他两个类是可变类。每次对String进行改变时，实际上是创建了新的对象，原来的对象会被GC回收掉。例如下面的代码，在执行第二句后，s实际上已经指向了新的String，其值为”abcd1”，而之前的String对象”abcd”由于没有引用指向它，所以后面会被GC回收。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;String s = &quot;abcd&quot;;
s = s+1;
System.out.print(s);// result : abcd1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h4 id=&quot;线程安全vs线程不安全&quot;&gt;线程安全vs线程不安全&lt;/h4&gt;
&lt;p&gt;线程安全：String, StringBuffer。&lt;/p&gt;

&lt;p&gt;线程不安全：StringBuilder。&lt;/p&gt;

&lt;h4 id=&quot;特殊的例子&quot;&gt;特殊的例子&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;String str = “This is only a” + “ simple” + “ test”;
StringBuilder builder = new StringBuilder(&quot;This is only a&quot;).append(&quot; simple&quot;).append(&quot; test&quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这个例子是对执行效率的补充。这里，生成str对象的速度简直太快了，而这个时候StringBuffer居然速度上根本一点都不占优势。其实这是JVM的一个把戏，实际上：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;String str = “This is only a” + “ simple” + “test”;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;其实就是：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;String str = “This is only a simple test”;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;所以不需要太多的时间了。但大家这里要注意的是，如果你的字符串是来自另外的String对象的话，速度就没那么快了，譬如：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;String str2 = “This is only a”;
String str3 = “ simple”;
String str4 = “ test”;
String str1 = str2 +str3 + str4;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这时候JVM会规规矩矩的按照原来的方式去做。&lt;/p&gt;

&lt;h3 id=&quot;如何比较stringbuffer字符串内容相等&quot;&gt;如何比较StringBuffer字符串内容相等&lt;/h3&gt;
&lt;p&gt;大家都知道String在比较内容相等时，用equals方法就行，这是因为String重写了Object的equals方法。但是StringBuffer没有重写equals方法，所以不能用equals来比较。&lt;/p&gt;

&lt;p&gt;所以我们一般用toString()把StringBuffer转换为String，再利用equals来比较。&lt;/p&gt;

&lt;h3 id=&quot;string重载与stringbuilder&quot;&gt;String重载”+”与StringBuilder&lt;/h3&gt;
&lt;p&gt;在Java中的String类已经重载的”+”。也就是说，字符串可以直接使用”+”进行连接。大家都知道，大量用”+”操作的时候，会产生很多垃圾，所以一般用StringBuilder或者StringBuffer。（这部分参考了Java编程思想的第13章字符串）&lt;/p&gt;

&lt;p&gt;而我们在用javap反编译的时候，发现编译时仍然将”+”转换成StringBuilder。因此，我们可以得出结论，在进行字符串连接时，实际上使用的是StringBuilder类的append()方法。&lt;/p&gt;

&lt;p&gt;但是编译器不会进行很多优化，每进行一次”+”，就会创建一个StringBuilder对象，虽然Java有垃圾回收器，但这个回收器的工作时间是不定的。如果不断产生这样的垃圾，那么仍然会占用大量的资源。解决这个问题的方法就是在程序中直接使用StringBuilder类来连接字符串。&lt;/p&gt;
</description>
        <pubDate>Wed, 24 May 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//Java-String</link>
        <guid isPermaLink="true">http://localhost:4000//Java-String</guid>
        
        
      </item>
    
      <item>
        <title>机器学习降维方法总结</title>
        <description>&lt;p&gt;降维在机器学习里面再正常不过了，这里总结了降维的一些方法，主要参考了陈利人老师的“数据分析领域中最为人称道的七种降维方法”（在微信公众号看到的，无法提供链接，有兴趣的可以搜索看原文）。不过这篇文章除了PCA，其他的降维方法多多少少有点特征工程的意思了。&lt;/p&gt;

&lt;h2 id=&quot;缺失值比率-missing-values-ratio&quot;&gt;缺失值比率 (Missing Values Ratio)&lt;/h2&gt;
&lt;p&gt;该方法的是基于包含太多缺失值的数据列包含有用信息的可能性较少。因此，可以将数据列缺失值大于某个阈值的列去掉。阈值越高，降维方法更为积极，即降维越少。&lt;/p&gt;

&lt;h2 id=&quot;低方差滤波&quot;&gt;低方差滤波&lt;/h2&gt;
&lt;p&gt;与上个方法相似，该方法假设数据列变化非常小的列包含的信息量少。因此，所有的数据列方差小的列被移除。需要注意的一点是：方差与数据范围相关的，因此在采用该方法前需要对数据做归一化处理。&lt;/p&gt;

&lt;h2 id=&quot;高相关滤波&quot;&gt;高相关滤波&lt;/h2&gt;
&lt;p&gt;高相关滤波认为当两列数据变化趋势相似时，它们包含的信息也显示。这样，使用相似列中的一列就可以满足机器学习模型。对于数值列之间的相似性通过计算相关系数来表示，对于名词类列的相关系数可以通过计算皮尔逊卡方值来表示。相关系数大于某个阈值的两列只保留一列。同样要注意的是：相关系数对范围敏感，所以在计算之前也需要对数据进行归一化处理。&lt;/p&gt;

&lt;h2 id=&quot;随机森林组合树&quot;&gt;随机森林/组合树&lt;/h2&gt;
&lt;p&gt;组合决策树通常又被成为随机森林，它在进行特征选择与构建有效的分类器时非常有用。一种常用的降维方法是对目标属性产生许多巨大的树，然后根据对每个属性的统计结果找到信息量最大的特征子集。例如，我们能够对一个非常巨大的数据集生成非常层次非常浅的树，每颗树只训练一小部分属性。如果一个属性经常成为最佳分裂属性，那么它很有可能是需要保留的信息特征。对随机森林数据属性的统计评分会向我们揭示与其它属性相比，哪个属性才是预测能力最好的属性。&lt;/p&gt;

&lt;h2 id=&quot;主成分分析-pca&quot;&gt;主成分分析 (PCA)&lt;/h2&gt;
&lt;p&gt;主成分分析是一个统计过程，该过程通过正交变换将原始的 n 维数据集变换到一个新的被称做主成分的数据集中。变换后的结果中，第一个主成分具有最大的方差值，每个后续的成分在与前述主成分正交条件限制下与具有最大方差。降维时仅保存前 m(m &amp;lt; n) 个主成分即可保持最大的数据信息量。需要注意的是主成分变换对正交向量的尺度敏感。数据在变换前需要进行归一化处理。同样也需要注意的是，新的主成分并不是由实际系统产生的，因此在进行 PCA 变换后会丧失数据的解释性。如果说，数据的解释能力对你的分析来说很重要，那么 PCA 对你来说可能就不适用了。&lt;/p&gt;

&lt;h2 id=&quot;反向特征消除&quot;&gt;反向特征消除&lt;/h2&gt;
&lt;p&gt;在该方法中，所有分类算法先用 n 个特征进行训练。每次降维操作，采用 n-1 个特征对分类器训练 n 次，得到新的 n 个分类器。将新分类器中错分率变化最小的分类器所用的 n-1 维特征作为降维后的特征集。不断的对该过程进行迭代，即可得到降维后的结果。第k 次迭代过程中得到的是 n-k 维特征分类器。通过选择最大的错误容忍率，我们可以得到在选择分类器上达到指定分类性能最小需要多少个特征。&lt;/p&gt;

&lt;h2 id=&quot;前向特征构造&quot;&gt;前向特征构造&lt;/h2&gt;
&lt;p&gt;前向特征构建是反向特征消除的反过程。在前向特征过程中，我们从 1 个特征开始，每次训练添加一个让分类器性能提升最大的特征。前向特征构造和反向特征消除都十分耗时。它们通常用于输入维数已经相对较低的数据集。&lt;/p&gt;

&lt;p&gt;除了本博客中提到的其中，还包括：随机投影(Random Projections)、非负矩阵分解(N0n-negative Matrix Factorization),自动编码(Auto-encoders),卡方检测与信息增益(Chi-square and information gain)， 多维标定(Multidimensional Scaling), 相关性分析(Coorespondence Analysis), 因子分析(Factor Analysis)、聚类(Clustering)以及贝叶斯模型(Bayesian Models)。&lt;/p&gt;
</description>
        <pubDate>Thu, 18 May 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93</link>
        <guid isPermaLink="true">http://localhost:4000//%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93</guid>
        
        
      </item>
    
      <item>
        <title>如何进行特征处理</title>
        <description>&lt;p&gt;参加kaggle的都知道，特征处理比跑模型重要的多，在特征处理上的时间也更多，这里总结一下常见的特征处理方法，在书上或者博客上看见一些比较好的处理特征的方法，我就总结在这里，并注明出处，持续更新。。。&lt;/p&gt;

&lt;p&gt;先看看特征工程的总体，下面这幅图来自&lt;a href=&quot;http://www.jianshu.com/p/01a2647b43bc&quot;&gt;一次kaggle的特征工程总结&lt;/a&gt;，后面的总结不像这幅图系统，但是我会做到尽量分类，尽量系统。
&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201705/798143-20170516111744244-1343315599.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;离散型特征&quot;&gt;离散型特征&lt;/h2&gt;
&lt;h3 id=&quot;离散型的值出现次数少&quot;&gt;离散型的值出现次数少&lt;/h3&gt;
&lt;p&gt;如果某一列是离散型特征，而且这一列有些值出现的次数非常小（经常要对离散型特征的值统计一下次数，才能判断多小才是小），我们可以把这些值统一给一个值，例如Rare，在kaggle泰坦尼克号里面，就可以这么对人姓名的title这么处理。&lt;/p&gt;

&lt;h3 id=&quot;去掉取值变化小的特征&quot;&gt;去掉取值变化小的特征&lt;/h3&gt;
&lt;p&gt;这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。摘自&lt;a href=&quot;http://blog.csdn.net/woaidapaopao/article/details/62461380&quot;&gt;干货：结合Scikit-learn介绍几种常用的特征选择方法 &lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;label-encoding-vs-one-hot-encoding&quot;&gt;Label Encoding vs One Hot Encoding&lt;/h3&gt;
&lt;p&gt;参加我的这篇博客&lt;a href=&quot;http://www.cnblogs.com/-Sai-/p/6708205.html&quot;&gt;Label Encoding vs One Hot Encoding&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;连续型特征&quot;&gt;连续型特征&lt;/h2&gt;
&lt;h3 id=&quot;连续型特征映射成离散型&quot;&gt;连续型特征映射成离散型&lt;/h3&gt;
&lt;p&gt;连续型特征在kaggle里常常被映射成离散型特征，具体的分析可以看我的博客&lt;a href=&quot;http://www.cnblogs.com/-Sai-/p/6707327.html&quot;&gt;机器学习模型为什么要将特征离散化&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;是否归一化或者标准化&quot;&gt;是否归一化或者标准化&lt;/h3&gt;
&lt;p&gt;一般来说，连续型特征常常要做归一化或者标准化处理，但是假如你把特征映射成了离散型特征，那这个归一化或标准化处理可做可不做（一般可做可不做，就是不做，不然要增加计算量）。&lt;/p&gt;

&lt;h2 id=&quot;缺失值处理&quot;&gt;缺失值处理&lt;/h2&gt;
&lt;h3 id=&quot;缺失值处理常见方法&quot;&gt;缺失值处理常见方法&lt;/h3&gt;
&lt;p&gt;用平均数、众数、K最近邻平均数等来赋值，要是离散型特征，可以直接用pandas的fillna()来填“missing”，然后做Label Encoding或者One Hot Encoding。&lt;/p&gt;

&lt;h2 id=&quot;特征选择&quot;&gt;特征选择&lt;/h2&gt;
&lt;h3 id=&quot;特征选择的方法&quot;&gt;特征选择的方法&lt;/h3&gt;
&lt;p&gt;这里的特征选择仅仅是筛选特征，不包括降维。&lt;/p&gt;

&lt;p&gt;按照周志华版的机器学习的第11章，他把特征选择的方法总结成了三类：过滤式，包裹式和嵌入式。（注：过滤式和包裹式的处理方式挺少见）&lt;/p&gt;

&lt;h3 id=&quot;过滤式&quot;&gt;过滤式&lt;/h3&gt;
&lt;p&gt;过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程和后续的学习器无关。选择过程有前向搜索和后向搜索，评价标准有子集的信息增益，相关统计量（周志华书）和拟合优度判定系数（ISLR）。&lt;/p&gt;

&lt;h3 id=&quot;包裹式&quot;&gt;包裹式&lt;/h3&gt;
&lt;p&gt;包裹式特征选择是把学习器的性能作为特征子集的评价标准。一般来说，包裹式特征选择比过滤式更好（因为过滤式特征选择不考虑后续的学习器的性能），但是计算开销上却比过滤式大得多。&lt;/p&gt;

&lt;h3 id=&quot;嵌入式&quot;&gt;嵌入式&lt;/h3&gt;
&lt;p&gt;嵌入式特征选择是将特征选择和学习器训练过程融合一体，两者在同一个优化过程中完成，常见的有L1正则化和L2正则化，但是L1正则化能更易获得一个稀疏的解。&lt;/p&gt;

&lt;h3 id=&quot;单变量特征选择&quot;&gt;单变量特征选择&lt;/h3&gt;
&lt;p&gt;单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验，皮尔逊相关系数等方式对特征进行测试。&lt;/p&gt;

&lt;p&gt;这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。摘自&lt;a href=&quot;http://blog.csdn.net/woaidapaopao/article/details/62461380&quot;&gt;干货：结合Scikit-learn介绍几种常用的特征选择方法 &lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;降维&quot;&gt;降维&lt;/h2&gt;
&lt;p&gt;###降维的方法
一般经过One Hot Encoding后，矩阵的维度可能会变的很大，此时一般需要进行降维的操作，常见的降维方法有PCA，更多的方法可以看看我的博客&lt;a href=&quot;http://www.cnblogs.com/-Sai-/p/6868534.html&quot;&gt;机器学习降维方法总结&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;特征构造&quot;&gt;特征构造&lt;/h2&gt;
&lt;h3 id=&quot;构造新特征的方法&quot;&gt;构造新特征的方法&lt;/h3&gt;
&lt;p&gt;泰坦尼克号比赛里面常用的一个构造新特征的方法是把兄弟姐妹的数量，伴侣的数据，父母子女的数量加起来，构造成一个新的特征：家庭的大小。&lt;/p&gt;

&lt;p&gt;除了上面的直接相加，还有构造多项式特征（特征相乘）等。&lt;/p&gt;
</description>
        <pubDate>Wed, 17 May 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86</link>
        <guid isPermaLink="true">http://localhost:4000//%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86</guid>
        
        
      </item>
    
      <item>
        <title>Java Web应用开发中的一些概念</title>
        <description>&lt;p&gt;最近在学习Java Web，发现Java Web的概念很多，而且各个概念之间的关系也挺复杂，本篇博客把这些关系总结于此，主要参考的博客附在文章末尾。&lt;/p&gt;

&lt;h2 id=&quot;概念&quot;&gt;概念&lt;/h2&gt;
&lt;h3 id=&quot;服务器&quot;&gt;服务器&lt;/h3&gt;
&lt;p&gt;服务器，硬件角度上说就是一台高性能的计算机。我们通常指的服务器其实应该是装有能够处理具体请求事务的服务器软件的计算机。比如最常见的www服务器、mail服务器、计费服务器、ftp服务器等等。很多时候人们常把诸如Tomcat、IIS、Weblogic 也称之为web服务器，其实这些只是用于开发、集成、部署和管理Web应用、网络应用和数据库应用的应用服务器软件。我们把遵守J2EE规范中的WEBAPPLICATION标准的WEB服务器就叫做J2EE中的WEB容器。&lt;/p&gt;

&lt;h3 id=&quot;web容器&quot;&gt;Web容器&lt;/h3&gt;
&lt;p&gt;所有的程序运行都需要有一个必要的运行环境。这个环境可以是软件，也可以是硬件，或者是软件和硬件的结合。比如说Windows操作系统需要运行在硬件基础上；Office软件需要运行在操作系统上。程序与运行环境之间会有一定的数据交换，比如操作系统会将运行指令传递给硬件，硬件也会将指令运行结果传递给操作系统。&lt;/p&gt;

&lt;p&gt;Java Web程序也需要一个运行环境才能够执行。这种运行Java Web程序的环境被称为Web容器，Java Web程序与Web容器之间存在数据交互。目前主要存在两种类型的Java Web容器：一种是独立的Java Web容器，在这种容器里面只能运行Web程序，这种容器一般也叫做Web服务器，如Tomcat等；另一种是与其他Java EE容器混合在一起的Web容器，Web容器负责运行Web程序，其他容器负责运行EJB等程序，如WebLogic等。&lt;/p&gt;

&lt;p&gt;当用户通过浏览器等Web客户端软件向服务器发出一个请求之后，首先接收到这个请求的是Web容器，Web容器会将请求信息封装到一个HttpServletRequest类型的Java对象中，并将对象传递给Java Web程序。对于每个请求，Web容器还会创建一个HttpServletResponse类型对象，该对象的作用是用来保存需要返回到浏览器等客户端的内容。&lt;/p&gt;

&lt;p&gt;Java Web程序可以从HttpServletRequest提取用户发出的请求信息，进行适当的处理之后，将处理结果放置到HttpServletResponse中，然或由Web容器解析HttpServletResponse对象内容，并将解析结果以浏览器能够识别的HTML等格式返回给浏览器等客户端。在这个过程中，HttpServletRequest和HttpServletResponse就如同两条渡船，负责在Web容器和Java Web程序之间传递信息。需要注意的是，这两个对象仅存在于Web容器和Java Web程序当中，与浏览器没有关系。下图是这个过程的示意图：
&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201705/798143-20170523230620904-819032297.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;web组件&quot;&gt;Web组件&lt;/h3&gt;
&lt;p&gt;Web组件(Web Component)是构成Web应用、能够对来自浏览器等客户端请求做出回应的基本单元。在Java Web应用中，所提供的Web组件有Servlet、JavaServer Pages(JSP)、JavaServer Faces(JSF)等。另外还可以使用其他类型Web应用同样使用的内容，比如HTML文件、静态图片等。事实上Java Web只提供了一种Web组件——Servlet，而JSP、JSF等都是基于Servlet的衍生技术。Servlet实质就是一个有特殊继承关系要求的Java类，理解Servlet就基本上完全理解Java Web技术了。&lt;/p&gt;

&lt;h3 id=&quot;web应用&quot;&gt;Web应用&lt;/h3&gt;
&lt;p&gt;Web应用可以认为是Web程序的另一个名称。定义就是运行在Web容器当中的，能够完成完整功能的应用程序。Java Web应用由一系列编译之后的Java类、静态图片、静态HTML文件、配置文件等构成。通过Java Web应用可以就收用户的输入和请求，并对请求信息进行加工处理、访问数据库、对用户的请求给出回应。&lt;/p&gt;

&lt;p&gt;开发Java Web应用程序处理需要遵守Web应用的一些特殊规范要求以外，与开发其他类型的应用程序并没有本质区别。目前Web应用主要有两种类型：一种是以展示为目的的Web应用，这种应用会以HTML等标记文本或者浏览器所能识别的媒体格式最为请求返回内容，用户可以通过浏览器等客户端操作这类应用。另一类是以服务为目的的Web应用，这类应用不提供图形操作界面，只提供方法调用结构，比如Web服务，该类应用一般只能被其他应用程序调用和访问。&lt;/p&gt;

&lt;h3 id=&quot;web模块&quot;&gt;Web模块&lt;/h3&gt;
&lt;p&gt;在Java EE技术体系中，除了Java Web以外，还包括EJB等其他技术。每个技术体系会运行在各自的容器当中，不同的部分组合在一起构成完整的Java EE应用，每个部分根据自己的特点被称为Java Web模块或者EJB模块。之所以把他们称为模块，是因为希望Java EE应用不同部分能像积木一样组合出更大、更复杂的应用出来，并且每个部分保证其独立性。不同的模块都有自己特定的目录结构要求。Java Web模块的目录结构要求如下。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;应用文档根目录。改目录是Java Web模块的最上层目录，组成Web模块的所有Web组件以及配置文件等都包含在该目录或者子目录下。在改目录下可以直接存放HTML文件，图片文件等。&lt;/li&gt;
  &lt;li&gt;WEB-INF目录。改目录位于应用文档根目录下，用来保存那些不希望被浏览器等客户端访问的文件，比如发布描述文件、编译之后的类文件。因为这些文件是由由Web容器进行解释或者执行的。浏览器不可以直接看到这些文件，只能看到这些文件的执行结果。&lt;/li&gt;
  &lt;li&gt;web.xml文件。该文件位于WEB-INF目录下，是Web应用的部署描述文件，用来与定义Web应用运行相关的信息，比如执行逻辑，Servlet等Web组件的访问路径等。改文件的格式必须符合所使用的Servlet版本的规格要求。&lt;/li&gt;
  &lt;li&gt;classes目录。该目录位于WEB-INF目录下，用来保存编译之后的Java类文件，比如Servlet、辅助类、其他业务处理类等。&lt;/li&gt;
  &lt;li&gt;tags目录。该目录位于WEB-INF目录下，用来保存那些标签库的标签描述文件。&lt;/li&gt;
  &lt;li&gt;lib目录。该目录位于WEB-INF目录下，用来保存Web应用运行所依赖的，经过打包之后的Java类文件。打包格式一般是.jar格式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于Java Web模块目录中WEB-INF、web.xml、classes、tags、lib这些内容以外，用户可以根据自己的需要在Web应用文档根目录下创建其他目录，比如可以为图片、不同用途的HTML或者JSP单独创建目录；还可以为不用的Java类在classes下创建包目录。如果在你的Web应用中没有使用到Servlet、过滤器、监听器等Java Web组件，可以没有web.xml文件；如果没有使用到标签，则可以没有tags目录。&lt;/p&gt;

&lt;p&gt;Web模块的发布工作，可以直接以目录的形式发布，也可以打包成jar格式文件进行发布。因为用途和内容的特殊性，打包文件的扩展名会采用.war而不是.jar。另外如果不是发布到独立的Web服务器上，而是发布到与其他Java EE一起使用的企业应用服务器上，还需要编写一个与企业应用服务器相关的配置文件，这个文件对于不同的企业应用服务器也不同，比如发布到glassfish服务器上，需要编写的文件时sun-web.xml。&lt;/p&gt;

&lt;h2 id=&quot;一些概念的对比&quot;&gt;一些概念的对比&lt;/h2&gt;
&lt;h3 id=&quot;java-servlet-vs-java-server-page&quot;&gt;Java Servlet vs Java Server Page&lt;/h3&gt;
&lt;p&gt;尽管JSP在本质上就是Servlet，JSP编译后就是一个“类servlet”，但是两者的创建方式不一样。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Servlet完全是Java程序代码构成，擅长于流程控制和事务处理，而通过Servlet来生成动态网页很不直观；对于静态的html标签，Servlet都必须使用页面输出流逐行输出&lt;/li&gt;
  &lt;li&gt;JSP由html代码和JSP标签构成可以方便地编写动态网页因此在实际应用中采用Servlet来控制业务流程，而采用JSP来生成动态页面。在struts框架中，jsp位于MVC设计模式的视图层，而servlet位于控制层。&lt;/li&gt;
  &lt;li&gt;Servlet中没有内置对象。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总之，JSP是Servlet的一种简化，使用JSP只需要完成程序员需要输出到客户端的内容，至于JSP中的Java脚本如果镶嵌到一个类中，由JSP容器完成，而Servlet则是个完整的java类，这个类的service方法用于生成对客户端的响应。&lt;/p&gt;

&lt;h3 id=&quot;tomcat与servlet&quot;&gt;Tomcat与Servlet&lt;/h3&gt;
&lt;p&gt;Servlet（小服务程序）是一个与协议无关的、跨平台的Web组件，它基于Java技术开发，由Servlet容器所管理。Servlet运行在服务器端，可以动态地扩展服务器的功能，并采用“请求一响应”模式提供Web服务。 Servlet的主要功能是交互式地浏览和修改数据，生成动态Web内容。&lt;/p&gt;

&lt;p&gt;当初在Apache开发时还未出现Servlet的概念，所以Apache不能内置支持Servlet。实
际上，除了Apache，其他许多Web服务器软件都不能直接支持Servlet。为了支持Servlet，
通常要单独开发程序，这种程序一般称为Web容器，它在Servlet的生命周期内包容和管理Servlet，是一个实时运行的外壳程序。运行时由Web服务器软件处理一般请求，并把Servlet调用传递给容器来处理。Tomcat就是满足这种需要的JSP/Servlet引擎，是Sun公司的JSP/Servlet的官方实现。Tomcat服务器接收客户端请求并作出响应的完整过程下图所示。&lt;/p&gt;

&lt;h3 id=&quot;tomcat与web服务器&quot;&gt;Tomcat与Web服务器&lt;/h3&gt;
&lt;p&gt;Tomcat是提供一个支持Servlet和JSP运行的容器。Servlet和JSP能根据实时需要，产生动态网页内容。而对于Web服务器来说，Apache仅仅支持静态网页，对于支持动态网页就会显得无能为力; Tomcat则既能为动态网页服务，同时也能为静态网页提供支持。尽管它没有通常的Web服务器快、功能也不如Web服务器丰富，但是Tomcat逐渐为支持静态内容不断扩充。大多数的Web服务器都是用底层语言编写如C，利用了相应平台的特征，因此用纯Java编写的Tomcat执行速度不可能与它们相提并论。&lt;/p&gt;

&lt;p&gt;一般来说，大的站点都是将Tomcat与Apache的结合，Apache负责接受所有来自客户端的HTTP请求，然后将Servlets和JSP的请求转发给Tomcat来处理。Tomcat完成处理后，将响应传回给Apache，最后Apache将响应返回给客户端。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/798143/201705/798143-20170524103004638-1447931283.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考博客&quot;&gt;参考博客&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://developer.51cto.com/art/201005/201999.htm&quot;&gt;Java Web应用开发中的一些概念&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/u013770825/article/details/20292613&quot;&gt;Tomcat与Servlet之间的关系&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Apr 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//Java-Web%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5</link>
        <guid isPermaLink="true">http://localhost:4000//Java-Web%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5</guid>
        
        
      </item>
    
      <item>
        <title>Scikit-Learn Tutorial</title>
        <description>&lt;p&gt;这是一篇翻译的博客，原文链接在&lt;a href=&quot;https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet#gs.BkcjXqk&quot;&gt;这里&lt;/a&gt;。这是我看的为数不多的介绍scikit-learn简介而全面的文章，特别适合入门。我这里把这篇文章翻译一下，英语好的同学可以直接看原文。&lt;/p&gt;

&lt;p&gt;大部分喜欢用Python来学习数据科学的人，应该听过scikit-learn，这个开源的Python库帮我们实现了一系列有关机器学习，数据处理，交叉验证和可视化的算法。其提供的接口非常好用。&lt;/p&gt;

&lt;p&gt;这就是为什么DataCamp(原网站)要为那些已经开始学习Python库却没有一个简明且方便的总结的人提供这个总结。（原文是cheat sheet，翻译过来就是小抄，我这里翻译成总结，感觉意思上更积极点）。或者你压根都不知道scikit-learn如何使用，那这份总结将会帮助你快速的了解其相关的基本知识，让你快速上手。&lt;/p&gt;

&lt;p&gt;你会发现，当你处理机器学习问题时，scikit-learn简直就是神器。&lt;/p&gt;

&lt;p&gt;这份scikit-learn总结将会介绍一些基本步骤让你快速实现机器学习算法，主要包括：读取数据，数据预处理，如何创建模型来拟合数据，如何验证你的模型以及如何调参让模型变得更好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/294/content_button-cheatsheet-scikit_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;总的来说，这份总结将会通过示例代码让你开始你的数据科学项目，你能立刻创建模型，验证模型，调试模型。（原文提供了pdf版的下载，内容和原文差不多）&lt;/p&gt;

&lt;h2 id=&quot;a-basic-example&quot;&gt;A Basic Example&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn import neighbors, datasets, preprocessing
&amp;gt;&amp;gt;&amp;gt; from sklearn.cross_validation import train_test_split
&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import accuracy_score
&amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
&amp;gt;&amp;gt;&amp;gt; X, y = iris.data[:, :2], iris.target
&amp;gt;&amp;gt;&amp;gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)
&amp;gt;&amp;gt;&amp;gt; scaler = preprocessing.StandardScaler().fit(X_train)
&amp;gt;&amp;gt;&amp;gt; X_train = scaler.transform(X_train)
&amp;gt;&amp;gt;&amp;gt; X_test = scaler.transform(X_test)
&amp;gt;&amp;gt;&amp;gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5)
&amp;gt;&amp;gt;&amp;gt; knn.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; y_pred = knn.predict(X_test)
&amp;gt;&amp;gt;&amp;gt; accuracy_score(y_test, y_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;（补充，这里看不懂不要紧，其实就是个小例子，后面会详细解答）&lt;/p&gt;

&lt;h2 id=&quot;loading-the-data&quot;&gt;Loading The Data&lt;/h2&gt;
&lt;p&gt;你的数据需要是numeric类型，然后存储成numpy数组或者scipy稀疏矩阵。我们也接受其他能转换成numeric数组的类型，比如Pandas的DataFrame。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; X = np.random.random((10,5))
&amp;gt;&amp;gt;&amp;gt; y = np.array(['M','M','F','F','M','F','M','M','F','F','F'])
&amp;gt;&amp;gt;&amp;gt; X[X &amp;lt; 0.7] = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;preprocessing-the-data&quot;&gt;Preprocessing The Data&lt;/h2&gt;
&lt;h3 id=&quot;standardization&quot;&gt;Standardization&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import StandardScaler
&amp;gt;&amp;gt;&amp;gt; scaler = StandardScaler().fit(X_train)
&amp;gt;&amp;gt;&amp;gt; standardized_X = scaler.transform(X_train)
&amp;gt;&amp;gt;&amp;gt; standardized_X_test = scaler.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;normalization&quot;&gt;Normalization&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import Normalizer
&amp;gt;&amp;gt;&amp;gt; scaler = Normalizer().fit(X_train)
&amp;gt;&amp;gt;&amp;gt; normalized_X = scaler.transform(X_train)
&amp;gt;&amp;gt;&amp;gt; normalized_X_test = scaler.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;binarization&quot;&gt;Binarization&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import Binarizer
&amp;gt;&amp;gt;&amp;gt; binarizer = Binarizer(threshold=0.0).fit(X)
&amp;gt;&amp;gt;&amp;gt; binary_X = binarizer.transform(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;encoding-categorical-features&quot;&gt;Encoding Categorical Features&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import LabelEncoder
&amp;gt;&amp;gt;&amp;gt; enc = LabelEncoder()
&amp;gt;&amp;gt;&amp;gt; y = enc.fit_transform(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;imputing-missing-values&quot;&gt;Imputing Missing Values&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;from sklearn.preprocessing import Imputer
&amp;gt;&amp;gt;&amp;gt;imp = Imputer(missing_values=0, strategy='mean', axis=0)
&amp;gt;&amp;gt;&amp;gt;imp.fit_transform(X_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;generating-polynomial-features&quot;&gt;Generating Polynomial Features&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import PolynomialFeatures)
&amp;gt;&amp;gt;&amp;gt; poly = PolynomialFeatures(5))
&amp;gt;&amp;gt;&amp;gt; oly.fit_transform(X))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;training-and-test-data&quot;&gt;Training And Test Data&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.cross_validation import train_test_split)
&amp;gt;&amp;gt;&amp;gt; X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;create-your-model&quot;&gt;Create Your Model&lt;/h2&gt;
&lt;h3 id=&quot;supervised-learning-estimators&quot;&gt;Supervised Learning Estimators&lt;/h3&gt;
&lt;h3 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.linear_model import LinearRegression)
&amp;gt;&amp;gt;&amp;gt; lr = LinearRegression(normalize=True))

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;support-vector-machines-svm&quot;&gt;Support Vector Machines (SVM)&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.svm import SVC)
&amp;gt;&amp;gt;&amp;gt; svc = SVC(kernel='linear'))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.naive_bayes import GaussianNB)
&amp;gt;&amp;gt;&amp;gt; gnb = GaussianNB())
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;knn&quot;&gt;KNN&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn import neighbors)
&amp;gt;&amp;gt;&amp;gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;unsupervised-learning-estimators&quot;&gt;Unsupervised Learning Estimators&lt;/h3&gt;
&lt;h3 id=&quot;principal-component-analysis-pca&quot;&gt;Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.decomposition import PCA)
&amp;gt;&amp;gt;&amp;gt; pca = PCA(n_components=0.95))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;k-means&quot;&gt;K Means&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.cluster import KMeans)
&amp;gt;&amp;gt;&amp;gt; k_means = KMeans(n_clusters=3, random_state=0))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&quot;model-fitting&quot;&gt;Model Fitting&lt;/h2&gt;
&lt;h3 id=&quot;supervised-learning&quot;&gt;Supervised learning&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; lr.fit(X, y))
&amp;gt;&amp;gt;&amp;gt; knn.fit(X_train, y_train))
&amp;gt;&amp;gt;&amp;gt; svc.fit(X_train, y_train))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; k_means.fit(X_train))
&amp;gt;&amp;gt;&amp;gt; pca_model = pca.fit_transform(X_train))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;prediction&quot;&gt;Prediction&lt;/h2&gt;
&lt;h3 id=&quot;supervised-estimators&quot;&gt;Supervised Estimators&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; y_pred = svc.predict(np.random.random((2,5))))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; y_pred = lr.predict(X_test))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; y_pred = knn.predict_proba(X_test))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;unsupervised-estimators&quot;&gt;Unsupervised Estimators&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; y_pred = k_means.predict(X_test))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;evaluate-your-models-performance&quot;&gt;Evaluate Your Model’s Performance&lt;/h2&gt;
&lt;h3 id=&quot;classification-metrics&quot;&gt;Classification Metrics&lt;/h3&gt;
&lt;h3 id=&quot;accuracy-score&quot;&gt;Accuracy Score&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; knn.score(X_test, y_test))
&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import accuracy_score)
&amp;gt;&amp;gt;&amp;gt; accuracy_score(y_test, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;classification-report&quot;&gt;Classification Report&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import classification_report)
&amp;gt;&amp;gt;&amp;gt; print(classification_report(y_test, y_pred)))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;confusion-matrix&quot;&gt;Confusion Matrix&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import confusion_matrix)
&amp;gt;&amp;gt;&amp;gt; print(confusion_matrix(y_test, y_pred)))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;regression-metrics&quot;&gt;Regression Metrics&lt;/h3&gt;
&lt;h3 id=&quot;mean-absolute-error&quot;&gt;Mean Absolute Error&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import mean_absolute_error)
&amp;gt;&amp;gt;&amp;gt; y_true = [3, -0.5, 2])
&amp;gt;&amp;gt;&amp;gt; mean_absolute_error(y_true, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;mean-squared-error&quot;&gt;Mean Squared Error&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import mean_squared_error)
&amp;gt;&amp;gt;&amp;gt; mean_squared_error(y_test, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;r2-score&quot;&gt;R2 Score&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import r2_score)
&amp;gt;&amp;gt;&amp;gt; r2_score(y_true, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;clustering-metrics&quot;&gt;Clustering Metrics&lt;/h3&gt;
&lt;h3 id=&quot;adjusted-rand-index&quot;&gt;Adjusted Rand Index&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import adjusted_rand_score)
&amp;gt;&amp;gt;&amp;gt; adjusted_rand_score(y_true, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;homogeneity&quot;&gt;Homogeneity&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import homogeneity_score)
&amp;gt;&amp;gt;&amp;gt; homogeneity_score(y_true, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;v-measure&quot;&gt;V-measure&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import v_measure_score)
&amp;gt;&amp;gt;&amp;gt; metrics.v_measure_score(y_true, y_pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;cross-validation&quot;&gt;Cross-Validation&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; print(cross_val_score(knn, X_train, y_train, cv=4))
&amp;gt;&amp;gt;&amp;gt; print(cross_val_score(lr, X, y, cv=2))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;tune-your-model&quot;&gt;Tune Your Model&lt;/h2&gt;
&lt;h3 id=&quot;grid-search&quot;&gt;Grid Search&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.grid_search import GridSearchCV
&amp;gt;&amp;gt;&amp;gt; params = {&quot;n_neighbors&quot;: np.arange(1,3), &quot;metric&quot;: [&quot;euclidean&quot;, &quot;cityblock&quot;]}
&amp;gt;&amp;gt;&amp;gt; grid = GridSearchCV(estimator=knn,param_grid=params)
&amp;gt;&amp;gt;&amp;gt; grid.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; print(grid.best_score_)
&amp;gt;&amp;gt;&amp;gt; print(grid.best_estimator_.n_neighbors)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;randomized-parameter-optimization&quot;&gt;Randomized Parameter Optimization&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.grid_search import RandomizedSearchCV
&amp;gt;&amp;gt;&amp;gt; params = {&quot;n_neighbors&quot;: range(1,5), &quot;weights&quot;: [&quot;uniform&quot;, &quot;distance&quot;]}
&amp;gt;&amp;gt;&amp;gt; rsearch = RandomizedSearchCV(estimator=knn,
   param_distributions=params,
   cv=4,
   n_iter=8,
   random_state=5)
&amp;gt;&amp;gt;&amp;gt; rsearch.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; print(rsearch.best_score_)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;going-further&quot;&gt;Going Further&lt;/h2&gt;
&lt;p&gt;学习完上面的例子后，你可以通过&lt;a href=&quot;https://www.datacamp.com/community/tutorials/machine-learning-python#gs.mDi3ekM&quot;&gt;our scikit-learn tutorial for beginners&lt;/a&gt;来学习更多的例子。另外你可以学习matplotlib来可视化数据。&lt;/p&gt;

&lt;p&gt;不要错过后续教程 &lt;a href=&quot;https://www.datacamp.com/community/blog/bokeh-cheat-sheet-python&quot;&gt;Bokeh cheat sheet&lt;/a&gt;, &lt;a href=&quot;https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.0dYnfNg&quot;&gt;the Pandas cheat sheet&lt;/a&gt; or &lt;a href=&quot;https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics#gs.Eekdm2k&quot;&gt;the Python cheat sheet for data science&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Fri, 07 Apr 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//Scikit-Learn-Tutorial</link>
        <guid isPermaLink="true">http://localhost:4000//Scikit-Learn-Tutorial</guid>
        
        
      </item>
    
      <item>
        <title>聚类算法深度详解</title>
        <description>&lt;p&gt;本博客主要内容来自机器之心翻译的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/26144586&quot;&gt;机器理解大数据的秘密：聚类算法深度详解&lt;/a&gt;。这篇文章是讲解聚类里难得一见的好文章，大家有兴趣可以阅读原文，我这里主要在原文的基础上写一些自己的总结，补充在原文后的括号里。&lt;/p&gt;

&lt;p&gt;本文主要介绍了三种聚类方法：K-均值聚类，层次聚类，图团体检测&lt;/p&gt;

&lt;h2 id=&quot;k均值聚类&quot;&gt;K均值聚类&lt;/h2&gt;
&lt;h3 id=&quot;何时使用&quot;&gt;何时使用？&lt;/h3&gt;
&lt;p&gt;当你事先知道你将找到多少个分组的时候。（这个就比较尴尬了，因为很多情况下，我们并不知道要聚多少个类）&lt;/p&gt;

&lt;h3 id=&quot;工作方式&quot;&gt;工作方式&lt;/h3&gt;
&lt;p&gt;该算法可以随机将每个观察（observation）分配到 k 类中的一类，然后计算每个类的平均。接下来，它重新将每个观察分配到与其最接近的均值的类别，然后再重新计算其均值。这一步不断重复，直到不再需要新的分配为止。（机器学习里面最简单的算法之一了，过程很简单）&lt;/p&gt;

&lt;h3 id=&quot;有效案例&quot;&gt;有效案例&lt;/h3&gt;
&lt;p&gt;假设有一组 9 位足球运动员，他们中每个人都在这一赛季进了一定数量的球（假设在 3-30 之间）。然后我们要将他们分成几组——比如 3 组。&lt;/p&gt;

&lt;p&gt;第一步：需要我们将这些运动员随机分成 3 组并计算每一组的均值。&lt;/p&gt;

&lt;p&gt;第 1 组
运动员 A（5 个球）、运动员 B（20 个球）、运动员 C（11 个球）
该组平均=(5 + 20 + 11) / 3 = 12
第 2 组
运动员 D（5 个球）、运动员 E（9 个球）、运动员 F（19 个球）
该组平均=11
第 3 组
运动员 G（30 个球）、运动员 H（3 个球）、运动员 I（15 个球）
该组平均=16
第二步：对于每一位运动员，将他们重新分配到与他们的分数最接近的均值的那一组；比如，运动员 A（5 个球）被重新分配到第 2 组（均值=11）。然后再计算新的均值。&lt;/p&gt;

&lt;p&gt;第 1 组（原来的均值=12）
运动员 C（11 个球）、运动员 E（9 个球）
新的平均=(11 + 9) / 2 = 10
第 2 组（原来的均值=11）
运动员 A（5 个球）、运动员 D（5 个球）、运动员 H（3 个球）
新的平均=4.33
第 3 组（原来的均值=16）
运动员 B（20 个球）、运动员 F（19 个球）、运动员 G（30 个球）、运动员 I（15 个球）
新的平均=21
不断重复第二步，直到每一组的均值不再变化。对于这个简单的任务，下一次迭代就能达到我们的目标。现在就完成了，你已经从原数据集得到了 3 个聚类！&lt;/p&gt;

&lt;p&gt;第 1 组（原来的均值=10）
运动员 C（11 个球）、运动员 E（9 个球）、运动员 I（15 个球）
最终平均=11.3
第 2 组（原来的均值=4.33）
运动员 A（5 个球）、运动员 D（5 个球）、运动员 H（3 个球）
最终平均=4.33
第 3 组（原来的均值=21）
运动员 B（20 个球）、运动员 F（19 个球）、运动员 G（30 个球）、
最终平均=23
通过这个例子，该聚类可能能够对应这些运动员在球场上的位置——比如防守、中场和进攻。K-均值在这里有效，是因为我们可以合理地预测这些数据会自然地落到这三个分组中。&lt;/p&gt;

&lt;p&gt;以这种方式，当给定一系列表现统计的数据时，机器就能很好地估计任何足球队的队员的位置——可用于体育分析，也能用于任何将数据集分类为预定义分组的其它目的的分类任务。
（K均值作为用的最广泛的聚类算法，例子太多了，而且我觉得原文的例子举的也比较一般。）&lt;/p&gt;

&lt;h3 id=&quot;更加细微的细节&quot;&gt;更加细微的细节&lt;/h3&gt;
&lt;p&gt;上面所描述的算法还有一些变体。最初的「种子」聚类可以通过多种方式完成。这里，我们随机将每位运动员分成了一组，然后计算该组的均值。这会导致最初的均值可能会彼此接近，这会增加后面的步骤。（随机选初始点会增加计算量）&lt;/p&gt;

&lt;p&gt;另一种选择种子聚类的方法是每组仅一位运动员，然后开始将其他运动员分配到与其最接近的组。这样返回的聚类是更敏感的初始种子，从而减少了高度变化的数据集中的重复性。但是，这种方法有可能减少完成该算法所需的迭代次数，因为这些分组实现收敛的时间会变得更少。（这不还是K均值吗。。）&lt;/p&gt;

&lt;p&gt;K-均值聚类的一个明显限制是你必须事先提供预期聚类数量的假设。目前也存在一些用于评估特定聚类的拟合的方法。比如说，聚类内平方和（Within-Cluster Sum-of-Squares）可以测量每个聚类内的方差。聚类越好，整体 WCSS 就越低。（讨论如何选K值又是一个大话题，而且衡量聚类好坏的标准不止WCSS一种。后面我找个时间总结一下，补充在这里）&lt;/p&gt;

&lt;h2 id=&quot;层次聚类&quot;&gt;层次聚类&lt;/h2&gt;
&lt;h3 id=&quot;何时使用-1&quot;&gt;何时使用？&lt;/h3&gt;
&lt;p&gt;当我们希望进一步挖掘观测数据的潜在关系，可以使用层次聚类算法。（感觉说了等于没说）&lt;/p&gt;

&lt;h3 id=&quot;工作方式-1&quot;&gt;工作方式&lt;/h3&gt;
&lt;p&gt;首先我们会计算距离矩阵（distance matrix），其中矩阵的元素（i，j）代表观测值 i 和 j 之间的距离度量。然后将最接近的两个观察值组为一对，并计算它们的平均值。通过将成对观察值合并成一个对象，我们生成一个新的距离矩阵。具体合并的过程即计算每一对最近观察值的均值，并填入新距离矩阵，直到所有观测值都已合并。（计算量很大。。）&lt;/p&gt;

&lt;h3 id=&quot;有效案例-1&quot;&gt;有效案例：&lt;/h3&gt;
&lt;p&gt;以下是关于鲸鱼或海豚物种分类的超简单数据集。作为受过专业教育的生物学家，我可以保证通常我们会使用更加详尽的数据集构建系统。现在我们可以看看这六个物种的典型体长。本案例中我们将使用 2 次重复步骤。&lt;/p&gt;

&lt;p&gt;Species          Initials  Length(m)
Bottlenose Dolphin     BD        3.0
Risso’s Dolphin        RD        3.6
Pilot Whale            PW        6.5
Killer Whale           KW        7.5
Humpback Whale         HW       15.0
Fin Whale              FW       20.0
步骤一：计算每个物种之间的距离矩阵，在本案例中使用的是欧氏距离（Euclidean distance），即数据点（data point）间的距离。你可以像在道路地图上查看距离图一样计算出距离。我们可以通过查看相关行和列的交叉点值来查阅任一两物种间的长度差。&lt;/p&gt;

&lt;p&gt;BD   RD   PW   KW   HW
RD  0.6
PW  3.5  2.9
KW  4.5  3.9  1.0
HW 12.0 11.4  8.5  7.5
FW 17.0 16.4 13.5 12.5  5.0
步骤二：将两个距离最近的物种挑选出来，在本案例中是宽吻海豚和灰海豚，他们平均体长达到了 3.3m。重复第一步，并再一次计算距离矩阵，但这一次将宽吻海豚和灰海豚的数据使用其均值长度 3.3m 代替。&lt;/p&gt;

&lt;p&gt;[BD, RD]   PW   KW   HW
PW       3.2
KW       4.2   1.0
HW      11.7   8.5  7.5
FW      16.7  13.5 12.5  5.0
接下来，使用新的距离矩阵重复步骤二。现在，最近的距离成了领航鲸与逆戟鲸，所以我们计算其平均长度（7.0m），并合并成新的一项。&lt;/p&gt;

&lt;p&gt;随后我们再重复步骤一，再一次计算距离矩阵，只不过现在将领航鲸与逆戟鲸合并成一项且设定长度为 7.0m。&lt;/p&gt;

&lt;p&gt;[BD, RD] [PW, KW]   HW
[PW, KW]      3.7
HW           11.7      8.0
FW           16.7     13.0   5.0
我们再一次使用现在的距离矩阵重复步骤 2。最近的距离（3.7m）出现在两个已经合并的项，现在我们将这两项合并成为更大的一项（均值为 5.2m）。&lt;/p&gt;

&lt;p&gt;[[BD, RD] , [PW, KW]]    HW
HW                   9.8
FW                  14.8   5.0
紧接着，我们再一次重复步骤 2，最小距离（5.0m）出现在座头鲸与长须鲸中，所以继续合并它们为一项，并计算均值（17.5m）。&lt;/p&gt;

&lt;p&gt;返回到步骤 1，计算新的距离矩阵，其中座头鲸与长须鲸已经合并为一项。&lt;/p&gt;

&lt;p&gt;[[BD, RD] , [PW, KW]]
[HW, FW]                  12.3
最后，重复步骤 2，距离矩阵中只存在一个值（12.3m），我们将所有的都合成为了一项，并且现在可以停止这一循环过程。先让我们看看最后的合并项。&lt;/p&gt;

&lt;p&gt;[[[BD, RD],[PW, KW]],[HW, FW]]
现在其有一个嵌套结构（参考 JSON），该嵌套结构能绘制成一个树状图。其和家族系谱图的读取方式相近。在树型图中，两个观察值越近，它们就越相似和密切相关。
&lt;img src=&quot;https://pic2.zhimg.com/v2-9c888bd47892aefc4946ec679529a759_b.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过树型图的结构，我们能更深入了解数据集的结构。在上面的案例中，我们看到了两个主要的分支，一个分支是 HW 和 FW，另一个是 BD、RD、PW、KW。&lt;/p&gt;

&lt;p&gt;在生物进化学中，通常会使用包含更多物种和测量的大型数据集推断这些物种之间的分类学关系。在生物学之外，层次聚类也在机器学习和数据挖掘中使用。&lt;/p&gt;

&lt;p&gt;重要的是，使用这种方法并不需要像 K-均值聚类那样设定分组的数量。你可以通过给定高度「切割」树型以返回分割成的集群。高度的选择可以通过几种方式进行，其取决于我们希望对数据进行聚类的分辨率。&lt;/p&gt;

&lt;p&gt;例如上图，如果我们在高度等于 10 的地方画一条线，就将两个主分支切开分为两个子图。如果我们从高度等于 2 的地方分割，就会生成三个聚类。&lt;/p&gt;

&lt;h3 id=&quot;更多细节&quot;&gt;更多细节&lt;/h3&gt;
&lt;p&gt;对于这里给出的层次聚类算法（hierarchical clustering algorithms），其有三个不同的方面。&lt;/p&gt;

&lt;p&gt;最根本的方法就是我们所使用的集聚（agglomerative）过程，通过该过程，我们从单个数据点开始迭代，将数据点聚合到一起，直到成为一个大型的聚类。另外一种（更高计算量）的方法从巨型聚类开始，然后将数据分解为更小的聚类，直到独立数据点。&lt;/p&gt;

&lt;p&gt;还有一些可以计算距离矩阵的方法，对于很多情况下，欧几里德距离（参考毕达哥拉斯定理）就已经够了，但还有一些可选方案在特殊的情境中更加适用。（距离的衡量也是一个大话题，不同的衡量方法，结果会很不一样，维基百科上列举了这五种：Euclidean distance，Squared Euclidean distance，Manhattan distance，maximum distance，Mahalanobis distance）&lt;/p&gt;

&lt;p&gt;最后，连接标准（linkage criterion）也可以改变。聚类根据它们不同的距离而连接，但是我们定义「近距离」的方式是很灵活的。在上面的案例中，我们通过测量每一聚类平均值（即形心（centroid））之间的距离，并与最近的聚类进行配对。但你也许会想用其他定义。（scikit-learn上提供了三种定义：“ward”, “complete”, “average”，具体参考我的博客&lt;a href=&quot;http://www.cnblogs.com/-Sai-/p/6666523.html&quot;&gt;层次聚类的连接标准&lt;/a&gt;，而且感觉这里连接标准没说清楚，我的理解是选择合并哪两个簇的方法）&lt;/p&gt;

&lt;p&gt;例如，每个聚类有几个离散点组成。我们可以将两个聚类间的距离定义为任意点间的最小（或最大）距离，就如下图所示。还有其他方法定义连接标准，它们可能适应于不同的情景。&lt;/p&gt;

&lt;h2 id=&quot;图团体检测&quot;&gt;图团体检测&lt;/h2&gt;
&lt;h3 id=&quot;何时使用-2&quot;&gt;何时使用？&lt;/h3&gt;
&lt;p&gt;当你的数据可以被表示为一个网络或图（graph）时。（这一节没怎么看懂，大家可以直接看原文）&lt;/p&gt;

&lt;h3 id=&quot;工作方式-2&quot;&gt;工作方式&lt;/h3&gt;
&lt;p&gt;图团体（graph community）通常被定义为一种顶点（vertice）的子集，其中的顶点相对于网络的其它部分要连接得更加紧密。存在多种用于识别图的算法，基于更具体的定义，其中包括（但不限于）：Edge Betweenness、Modularity-Maximsation、Walktrap、Clique Percolation、Leading Eigenvector……&lt;/p&gt;

&lt;h3 id=&quot;有效案例-2&quot;&gt;有效案例&lt;/h3&gt;
&lt;p&gt;图论是一个研究网络的数学分支，参考之前一篇文章《想了解概率图模型？你要先理解图论的基本定义与形式》(也是机器之心翻译的)。使用图论的方法，我们可以将复杂系统建模成为「顶点（vertice）」和「边（edge）」的抽象集合。&lt;/p&gt;

&lt;p&gt;也许最直观的案例就是社交网络。其中的顶点表示人，连接顶点的边表示他们是朋友或互粉的用户。&lt;/p&gt;

&lt;p&gt;但是，要将一个系统建模成一个网络，你必须要找到一种有效连接各个不同组件的方式。将图论用于聚类的一些创新应用包括：对图像数据的特征提取、分析基因调控网络（gene regulatory networks）。&lt;/p&gt;

&lt;p&gt;下面给出了一个入门级的例子，这是一个简单直接的图，展示了我最近浏览过的 8 个网站，根据他们的维基百科页面中的链接进行了连接。这个数据很简单，你可以人工绘制，但对于更大规模的项目，更快的方式是编写 Python 脚本。这里是我写的一个：https://raw.githubusercontent.com/pg0408/Medium-articles/master/graph_maker.py&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-75a199d71c268cc30bd3832b42842843_b.png&quot; alt=&quot;&quot; /&gt;
这些顶点的颜色表示了它们的团体关系，大小是根据它们的中心度（centrality）确定的。可以看到谷歌和 Twitter 是最中心的吧？&lt;/p&gt;

&lt;p&gt;另外，这些聚类在现实生活中也很有意义（一直是一个重要的表现指标）。黄色顶点通常是参考/搜索网站，蓝色顶点全部是在线发布网站（文章、微博或代码），而橙色顶点是 YouTube 和 PayPal——因为 YouTube 是由前 PayPal 员工创立的。机器还算总结得不错！&lt;/p&gt;

&lt;p&gt;除了用作一种有用的可视化大系统的方式，网络的真正力量是它们的数学分析能力。让我们将上面图片中的网络翻译成更数学的形式吧。下面是该网络的邻接矩阵（adjacency matrix）：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         GH  Gl M  P  Q  T  W  Y
GitHub    0  1  0  0  0  1  0  0  
Google    1  0  1  1  1  1  1  1
Medium    0  1  0  0  0  1  0  0
PayPal    0  1  0  0  0  1  0  1
Quora     0  1  0  0  0  1  1  0
Twitter   1  1  1  1  1  0  0  1
Wikipedia 0  1  0  0  1  0  0  0
YouTube   0  1  0  1  0  1  0  0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;每行和每列的交点处的值表示对应的顶点对之间是否存在边。比如说，在 Medium 和 Twitter 之间有一条边，所以它们的行列交点是 1。类似地，Medium 和 PayPal 之间没有边，所以它们的行列交点是 0.&lt;/p&gt;

&lt;p&gt;该邻接矩阵编码了该网络的所有属性——其给了我们开启所有有价值的见解的可能性的钥匙。首先，每一行或每一列的数字相加都能给你关于每个顶点的程度（degree）——即它连接到了多少个其它顶点，这个数字通常用字母 k 表示。类似地，将每个顶点的 degree 除以 2，则能得到边的数量，也称为链接（link），用 L 表示。行/列的数量即是该网络中顶点的数量，称为节点（node），用 N 表示。&lt;/p&gt;

&lt;p&gt;只需要知道 k、L 和 N 以及该邻接矩阵 A 中每个单元的值，就能让我们计算出该网络的任何给定聚类的模块性（modularity）。&lt;/p&gt;

&lt;p&gt;假设我们已经将该网络聚类成了一些团体。我们就可以使用该模块性分数来评估这个聚类的质量。分数更高表示我们将该网络分割成了「准确的（accurate）」团体，而低分则表示我们的聚类更接近随机。如下图所示：
&lt;img src=&quot;https://pic4.zhimg.com/v2-0886f1b1218d3c69c7e0b88ff51a136f_b.png&quot; alt=&quot;&quot; /&gt;
模块性（modularity）是用于测量分区的「质量」的一种标准&lt;/p&gt;

&lt;p&gt;模块性可以使用以下公式进行计算：
&lt;img src=&quot;https://pic4.zhimg.com/v2-95d8e845a1f8d1d9c916255d631dbddf_b.png&quot; alt=&quot;&quot; /&gt;
这个公式有点复杂，但我们分解它，让我们可以更好地理解。&lt;/p&gt;

&lt;p&gt;M 就是我们要计算的模块性。&lt;/p&gt;

&lt;p&gt;1/2L 告诉我们将后面的部分除以 2L，即网络中边的数量的两倍。&lt;/p&gt;

&lt;p&gt;Σ 符号表示求和，并且在该邻接矩阵 A 中的每一行和列上进行迭代。如果你对这个符号不熟悉，可以将 i, j = 1 和 N 理解成编程语言中的 for-loop。在 Python 里面，可以写成这样：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sum = 0
for i in range(1,N):
    for j in range(1,N):
        ans = #stuff with i and j as indices
        sum += ans
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;代码里面的 #stuff with i and j（带有 i 和 j 的那一坨）是什么？&lt;/p&gt;

&lt;p&gt;括号中的内容表示从 A_ij 减去 ( k_i k_j ) / 2L。&lt;/p&gt;

&lt;p&gt;A_ij 就是指该邻接矩阵中第 i 行、第 j 列的值。&lt;/p&gt;

&lt;p&gt;k_i 和 k_j 是指每个顶点的 degree——可以通过将每一行和每一列的项加起来而得到。两者相乘再除以 2L 表示当该网络是随机分配的时候顶点 i 和 j 之间的预期边数。&lt;/p&gt;

&lt;p&gt;整体而言，括号中的项表示了该网络的真实结构和随机组合时的预期结构之间的差。研究它的值可以发现，当 A_ij = 1 且 ( k_i k_j ) / 2L 很小时，其返回的值最高。这意味着，当在定点 i 和 j 之间存在一个「非预期」的边时，得到的值更高。&lt;/p&gt;

&lt;p&gt;最后，我们再将括号中的项和 δc_i, c_j 相乘。δc_i, c_j 就是大名鼎鼎但基本无害的克罗内克 δ 函数（Kronecker-delta function）。下面是其 Python 解释：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def Kronecker_Delta(ci, cj):
    if ci == cj:
        return 1
    else:
        return 0
Kronecker_Delta(&quot;A&quot;,&quot;A&quot;)    #returns 1
Kronecker_Delta(&quot;A&quot;,&quot;B&quot;)    #returns 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;是的，就是那么简单。克罗内克 δ 函数与两个参数，如何这两个参数相等则返回 1，如何不等，则返回 0.&lt;/p&gt;

&lt;p&gt;也就是说，如果顶点 i 和 j 已经被放进了同一个聚类，那么δc_i, c_j = 1；否则它们不在同一个聚类，函数返回 0.&lt;/p&gt;

&lt;p&gt;当我们将括号中的项与克罗内克 δ 函数相乘时，我们发现对于嵌套求和 Σ，当有大量「意外的（unexpected）」连接顶点的边被分配给同一个聚类时，其结果是最高的。因此，模块性是一种用于衡量将图聚类成不同的团体的程度的方法。&lt;/p&gt;

&lt;p&gt;除以 2L 将模块性的上限值设置成了 1。模块性接近或小于 0 表示该网络的当前聚类没有用处。模块性越高，该网络聚类成不同团体的程度就越好。通过是模块性最大化，我们可以找到聚类该网络的最佳方法。&lt;/p&gt;

&lt;p&gt;注意我们必须预定义图的聚类方式，才能找到评估一个聚类有多好的方法。不幸的是，使用暴力计算的方式来尝试各种可能以寻找最高模块性分数的聚类方式需要大量计算，即使在一个有限大小的样本上也是不可能的。&lt;/p&gt;

&lt;p&gt;组合学（combinatorics）告诉我们对于一个仅有 8 个顶点的网络，就存在 4140 种不同的聚类方式。16 个顶点的网络的聚类方式将超过 100 亿种。32 个顶点的网络的可能聚类方式更是将超过 128 septillion（10^21）种；如果你的网络有 80 个顶点，那么其可聚类的方式的数量就已经超过了可观测宇宙中的原子数量。&lt;/p&gt;

&lt;p&gt;因此，我们必须求助于一种启发式的方法，该方法在评估可以产生最高模块性分数的聚类上效果良好，而且并不需要尝试每一种可能性。这是一种被称为 Fast-Greedy Modularity-Maximization（快速贪婪模块性最大化）的算法，这种算法在一定程度上类似于上面描述的 agglomerative hierarchical clustering algorithm（集聚层次聚类算法）。只是 Mod-Max 并不根据距离（distance）来融合团体，而是根据模块性的改变来对团体进行融合。&lt;/p&gt;

&lt;p&gt;下面是其工作方式：&lt;/p&gt;

&lt;p&gt;首先初始分配每个顶点到其自己的团体，然后计算整个网络的模块性 M。&lt;/p&gt;

&lt;p&gt;第 1 步要求每个团体对（community pair）至少被一条单边链接，如果有两个团体融合到了一起，该算法就计算由此造成的模块性改变 ΔM。&lt;/p&gt;

&lt;p&gt;第 2 步是取 ΔM 出现了最大增长的团体对，然后融合。然后为这个聚类计算新的模块性 M，并记录下来。&lt;/p&gt;

&lt;p&gt;重复第 1 步和 第 2 步——每一次都融合团体对，这样最后得到 ΔM 的最大增益，然后记录新的聚类模式及其相应的模块性分数 M。&lt;/p&gt;

&lt;p&gt;当所有的顶点都被分组成了一个巨型聚类时，就可以停止了。然后该算法会检查这个过程中的记录，然后找到其中返回了最高 M 值的聚类模式。这就是返回的团体结构。&lt;/p&gt;

&lt;h3 id=&quot;更多细节-1&quot;&gt;更多细节&lt;/h3&gt;

&lt;p&gt;哇！这个过程真是有太多计算了，至少对我们人类而言是这样。图论中存在很多计算难题，常常是 NP-hard 问题——但其也在为复杂系统和数据集提供有价值的见解上具有出色的潜力。Larry Page 就知道这一点，其著名的 PageRank 算法就是完全基于图论的——该算法在帮助谷歌在不到十年之内从创业公司成长为近乎世界主宰的过程中立下了汗马功劳。&lt;/p&gt;

&lt;p&gt;团体检测（community detection）是现在图论中一个热门的研究领域，也存在很多可替代 Modularity-Maximization（尽管很有用，但也有缺点）的方法。&lt;/p&gt;

&lt;p&gt;首先，它的聚集方式从指定尺寸的小团体开始，逐渐转向越来越大的。这被称为分辨率极限（resolution limit）——该算法不会搜索特定尺寸以下的团体。另一个挑战则是超越一个显著波峰的表现，Mod-Max 方法趋向于制造一个由很多高模块化分数组成的「高原」，这有时会导致难以确定最大分数。&lt;/p&gt;

&lt;p&gt;其他算法使用不同的方式来确定团体。Edge-Betweenness 是一个分裂算法，把所有顶点聚合到一个大集群中。它会持续迭代去除网络中「最不重要」的边缘数据，直到所有顶点都被分开为止。这一过程产生了层级结构，其中类似的顶点在结构中互相靠近。&lt;/p&gt;

&lt;p&gt;另一种算法是 Clique Percolation，它考虑了图团体之间可能的重叠。而另外一些算法基于图中的随机游动，还有谱聚类（spectral clustering）算法：从邻接矩阵及派生矩阵的特征分解开始。这些方法被应用于特征提取任务，如计算机视觉。&lt;/p&gt;

&lt;p&gt;给出每个算法的深入应用实例超出了本介绍的探究范围。从数据中提取可用信息的有效方法在数十年前还是难以触及的事物，但现在已经成为了非常活跃的研究领域。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;希望本文能对你有所启发，让你更好地理解机器如何了解大数据。未来是高速变革的，其中的许多变化将会由下一代或两代中有能力的技术所驱动。&lt;/p&gt;

&lt;p&gt;就像导语提到的，机器学习是一个非常有前景的研究领域，其中有大量复杂的问题需要以准确、有效的方式解决。对人类来说轻而易举的任务在由机器完成的时候就需要创新性的解决方案。&lt;/p&gt;

&lt;p&gt;在此领域中，我们仍有大量的任务需要完成，无论谁为下一个重大突破贡献力量，无疑都会得到慷慨的回报。或许正在阅读这篇文章的某个人就会成为下一个强大的算法发明者？所有伟大想法都是从零开始的。&lt;/p&gt;
</description>
        <pubDate>Wed, 05 Apr 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%B7%B1%E5%BA%A6%E8%AF%A6%E8%A7%A3</link>
        <guid isPermaLink="true">http://localhost:4000//%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%B7%B1%E5%BA%A6%E8%AF%A6%E8%A7%A3</guid>
        
        
      </item>
    
      <item>
        <title>数据挖掘书籍推荐</title>
        <description>&lt;p&gt;这篇博客主要总结一下数据挖掘、数据分析领域相关书籍，主要参考了知乎上的问题&lt;a href=&quot;https://www.zhihu.com/question/20757000&quot;&gt;在数据分析、挖掘方面，有哪些好书值得推荐&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;首先推荐周志华写的机器学习。我最近也在读这本书，优点是适合入门，知识大而全，缺点是每个知识点介绍的不深入（这也没办法，要是面面俱到，一本书根本写不完）。&lt;/p&gt;
&lt;h2 id=&quot;入门读物&quot;&gt;入门读物&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;深入浅出数据分析。这书挺简单的，基本的内容都涉及了，说得也比较清楚，最后谈到了R是大加分。难易程度：非常易。&lt;/li&gt;
  &lt;li&gt;啤酒与尿布。通过案例来说事情，而且是最经典的例子。难易程度：非常易。&lt;/li&gt;
  &lt;li&gt;数据之美。一本介绍性的书籍，每章都解决一个具体的问题，甚至还有代码，对理解数据分析的应用领域和做法非常有帮助。难易程度：易。&lt;/li&gt;
  &lt;li&gt;数学之美。吴军博士写的，作为科普读物还不错。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;数据分析&quot;&gt;数据分析&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;SciPy and NumPy。numpy和scipy很好很强大&lt;/li&gt;
  &lt;li&gt;Python for Data Analysis。&lt;/li&gt;
  &lt;li&gt;Bad Data Handbook。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;数据挖掘入门书籍&quot;&gt;数据挖掘入门书籍&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;集体智慧编程。经典书籍，入门必读。&lt;/li&gt;
  &lt;li&gt;机器学习实战。理论很好，代码质量一般般。优点是让你看看如何实现这些算法，缺点是书中的代码几乎用不上。&lt;/li&gt;
  &lt;li&gt;数据挖掘导论。研究生期间的教材，通俗易懂，习题很赞。&lt;/li&gt;
  &lt;li&gt;Machine Learning for Hackers。算法用R实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;中阶&quot;&gt;中阶&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Introduction to Semi-Supervised Learning&lt;/li&gt;
  &lt;li&gt;Learning to Rank for Information Retrieval&lt;/li&gt;
  &lt;li&gt;Learning to Rank for Information Retrieval and Natural Language Processing&lt;/li&gt;
  &lt;li&gt;推荐系统实践。推荐系统入门首选&lt;/li&gt;
  &lt;li&gt;Natural Language Processing with Python。NLP 经典，其实主要是讲NLTK包&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;高阶&quot;&gt;高阶&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The Elements of Statistical Learning。很难，啃完不容易。&lt;/li&gt;
  &lt;li&gt;统计学习方法。李航老师的扛鼎之作，强烈推荐。&lt;/li&gt;
  &lt;li&gt;Machine Learning。作者Kevin Murrphy教授是机器学习领域中年少有为的代表。这书是他的集大成之作，写完之后，就去Google了，产学研结合，没有比这个更好的了。&lt;/li&gt;
  &lt;li&gt;Pattern Recognition And Machine Learning。PRML地位不解释。&lt;/li&gt;
  &lt;li&gt;Bayesian Reasoning and Machine Learning。Bayesian学派的书，里面的内容非常多，有一张图将机器学习中设计算法的关系总结了一下，很棒。&lt;/li&gt;
  &lt;li&gt;Probabilistic Graphical Models。非常非常难。&lt;/li&gt;
  &lt;li&gt;Convex Optimization (豆瓣) 凸优化中最好的教材，没有之一了。&lt;/li&gt;
  &lt;li&gt;Learning from data。林轩田老师作品。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;写在后面，看了肖博士的答案，确实比其他答案好不少，专业且全面，而且有针对性。除了书籍，后续我会补充一些公开课资源。&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Jan 2017 06:17:00 +0800</pubDate>
        <link>http://localhost:4000//%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B9%A6%E7%B1%8D%E6%8E%A8%E8%8D%90</link>
        <guid isPermaLink="true">http://localhost:4000//%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B9%A6%E7%B1%8D%E6%8E%A8%E8%8D%90</guid>
        
        
      </item>
    
  </channel>
</rss>
