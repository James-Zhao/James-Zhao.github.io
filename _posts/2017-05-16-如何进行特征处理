---
layout: post
title:  如何进行特征处理
date:   2017-06-04 22:17:00
---
参加kaggle的都知道，特征处理比跑模型重要的多，在特征处理上的时间也更多，这里总结一下常见的特征处理方法，在书上或者博客上看见一些比较好的处理特征的方法，我就总结在这里，并注明出处，持续更新。。。

先看看特征工程的总体，下面这幅图来自[一次kaggle的特征工程总结](http://www.jianshu.com/p/01a2647b43bc)，后面的总结不像这幅图系统，但是我会做到尽量分类，尽量系统。
![](http://images2015.cnblogs.com/blog/798143/201705/798143-20170516111744244-1343315599.png)



## 离散型特征
### 离散型的值出现次数少
如果某一列是离散型特征，而且这一列有些值出现的次数非常小（经常要对离散型特征的值统计一下次数，才能判断多小才是小），我们可以把这些值统一给一个值，例如Rare，在kaggle泰坦尼克号里面，就可以这么对人姓名的title这么处理。

### 去掉取值变化小的特征
这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。摘自[干货：结合Scikit-learn介绍几种常用的特征选择方法 ](http://blog.csdn.net/woaidapaopao/article/details/62461380)


### Label Encoding vs One Hot Encoding
参加我的这篇博客[Label Encoding vs One Hot Encoding](http://www.cnblogs.com/-Sai-/p/6708205.html)

## 连续型特征
### 连续型特征映射成离散型
连续型特征在kaggle里常常被映射成离散型特征，具体的分析可以看我的博客[机器学习模型为什么要将特征离散化](http://www.cnblogs.com/-Sai-/p/6707327.html)。

### 是否归一化或者标准化
一般来说，连续型特征常常要做归一化或者标准化处理，但是假如你把特征映射成了离散型特征，那这个归一化或标准化处理可做可不做（一般可做可不做，就是不做，不然要增加计算量）。

## 缺失值处理
### 缺失值处理常见方法
用平均数、众数、K最近邻平均数等来赋值，要是离散型特征，可以直接用pandas的fillna()来填“missing”，然后做Label Encoding或者One Hot Encoding。

## 特征选择
### 特征选择的方法
这里的特征选择仅仅是筛选特征，不包括降维。

按照周志华版的机器学习的第11章，他把特征选择的方法总结成了三类：过滤式，包裹式和嵌入式。（注：过滤式和包裹式的处理方式挺少见）

### 过滤式
过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程和后续的学习器无关。选择过程有前向搜索和后向搜索，评价标准有子集的信息增益，相关统计量（周志华书）和拟合优度判定系数（ISLR）。

### 包裹式
包裹式特征选择是把学习器的性能作为特征子集的评价标准。一般来说，包裹式特征选择比过滤式更好（因为过滤式特征选择不考虑后续的学习器的性能），但是计算开销上却比过滤式大得多。


### 嵌入式
嵌入式特征选择是将特征选择和学习器训练过程融合一体，两者在同一个优化过程中完成，常见的有L1正则化和L2正则化，但是L1正则化能更易获得一个稀疏的解。

### 单变量特征选择
单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验，皮尔逊相关系数等方式对特征进行测试。

这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。摘自[干货：结合Scikit-learn介绍几种常用的特征选择方法 ](http://blog.csdn.net/woaidapaopao/article/details/62461380)。
## 降维
###降维的方法
一般经过One Hot Encoding后，矩阵的维度可能会变的很大，此时一般需要进行降维的操作，常见的降维方法有PCA，更多的方法可以看看我的博客[机器学习降维方法总结](http://www.cnblogs.com/-Sai-/p/6868534.html)。

## 特征构造
### 构造新特征的方法
泰坦尼克号比赛里面常用的一个构造新特征的方法是把兄弟姐妹的数量，伴侣的数据，父母子女的数量加起来，构造成一个新的特征：家庭的大小。

除了上面的直接相加，还有构造多项式特征（特征相乘）等。
